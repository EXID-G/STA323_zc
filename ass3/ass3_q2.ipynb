{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.executor.memory\", \"16g\").config(\"spark.driver.memory\", \"16g\").config(\"spark.sql.shuffle.partitions\", \"3\").appName(\"ass3_q2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|            x|           y|            z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "|1424686735175|1424686733176178965|nexus4_1|   35|nexus4|   g|stand| 0.0014038086|   5.0354E-4|-0.0124053955|\n",
      "|1424686735378|1424686733382813486|nexus4_1|   76|nexus4|   g|stand|-0.0039367676| 0.026138306|  -0.01133728|\n",
      "|1424686735577|1424686733579072031|nexus4_1|  115|nexus4|   g|stand|  0.003540039|-0.034744263| -0.019882202|\n",
      "|1424686735779|1424688581834321412|nexus4_2|  163|nexus4|   g|stand|  0.002822876| 0.005584717|  0.017318726|\n",
      "|1424686735982|1424688582035859498|nexus4_2|  203|nexus4|   g|stand| 0.0017547607|-0.018981934| -0.022201538|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"data/activity-data/part-00000-tid-730451297822678341-1dda7027-2071-4d73-a0e2-7fb6a91e1d1f-0-c000.json\")\n",
    "dataSchema = static.schema\n",
    "static.printSchema()\n",
    "static.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "long overflow -> ref: https://stackoverflow.com/search?q=%5Bapache-spark%5Dstream+convert+timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "|Arrival_Time |Creation_Time             |Device  |Index|Model |User|gt   |x            |y           |z            |\n",
      "+-------------+--------------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "|1424686735175|2015-02-23 10:18:53.176179|nexus4_1|35   |nexus4|g   |stand|0.0014038086 |5.0354E-4   |-0.0124053955|\n",
      "|1424686735378|2015-02-23 10:18:53.382813|nexus4_1|76   |nexus4|g   |stand|-0.0039367676|0.026138306 |-0.01133728  |\n",
      "|1424686735577|2015-02-23 10:18:53.579072|nexus4_1|115  |nexus4|g   |stand|0.003540039  |-0.034744263|-0.019882202 |\n",
      "|1424686735779|2015-02-23 10:49:41.834321|nexus4_2|163  |nexus4|g   |stand|0.002822876  |0.005584717 |0.017318726  |\n",
      "|1424686735982|2015-02-23 10:49:42.035859|nexus4_2|203  |nexus4|g   |stand|0.0017547607 |-0.018981934|-0.022201538 |\n",
      "+-------------+--------------------------+--------+-----+------+----+-----+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.withColumn(\"Creation_Time\",to_timestamp(col(\"Creation_Time\")/1000000000)).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil  \n",
    "if os.path.exists('checkpoint/activity-data/offsets'): \n",
    "    shutil.rmtree('checkpoint/activity-data/offsets')     # 强制删除文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 09:38:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\",10).json(\"data/activity-data/*.json\")\n",
    "checkpointDir = \"checkpoint/activity-data\"\n",
    "\n",
    "activityCounts = streaming.withColumn(\"Creation_Time\",to_timestamp(col(\"Creation_Time\")/1000000000)).withWatermark(\"Creation_Time\", \"1 minute\").groupBy(\"user\",window(\"Creation_Time\",\"6 minutes\",\"3 minutes\")).count()\n",
    "\n",
    "#* with checkpoint\n",
    "activityQuery = activityCounts.writeStream.queryName(\"activity_query\").format(\"memory\").outputMode(\"update\").trigger(processingTime=\"2 seconds\").option(\"checkpointLocation\", checkpointDir).start()\n",
    "\n",
    "# * no checkpoint\n",
    "# activityQuery = activityCounts.writeStream.queryName(\"activity_query\").format(\"memory\").outputMode(\"update\").trigger(processingTime=\"2 seconds\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 如果没有checkpoint，数据会从头开始跑，每次程序的启动 重复运行下面的查询语句结果会不一样；\n",
    "\n",
    "如果有checkpoint，spark强制让我删除offset，但是offset存储偏移量的（这里很奇怪，每次运行前让我删除offset，否则报错），删除之后checkpoint可以说是没有用了，因为数据会从头开始跑，每次程序的启动 重复运行下面的查询语句结果会不一样；但是最后的效果是，我每次查询后的结果都是一样的，结果都是一样的，就像是checkpoint起到了作用（数据全部跑完，不用再更新查询的结果），但是理论上删除了offset后，是不会存储前面查询的呀。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+-----+\n",
      "|user|window                                    |count|\n",
      "+----+------------------------------------------+-----+\n",
      "|a   |{2015-02-22 00:36:00, 2015-02-22 00:42:00}|9    |\n",
      "|a   |{2015-02-22 00:36:00, 2015-02-22 00:42:00}|16   |\n",
      "|a   |{2015-02-22 00:39:00, 2015-02-22 00:45:00}|9    |\n",
      "|a   |{2015-02-22 00:39:00, 2015-02-22 00:45:00}|16   |\n",
      "|g   |{2015-02-23 10:15:00, 2015-02-23 10:21:00}|4696 |\n",
      "+----+------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM activity_query order by window\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 09:38:32 ERROR TorrentBroadcast: Store broadcast broadcast_120 fail, remove all pieces of the broadcast\n"
     ]
    }
   ],
   "source": [
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different checkpoints directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil  \n",
    "if os.path.exists('checkpoint/activity-data-append-parquet/offsets'): \n",
    "    shutil.rmtree('checkpoint/activity-data-append-parquet/offsets')     # 强制删除文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 09:41:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "parquetOutputPath = \"output/activity-data\"\n",
    "activityQuery2 = activityCounts.writeStream \\\n",
    "    .queryName(\"activity_query2\")\\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/activity-data-append-parquet\") \\\n",
    "    .option(\"path\", parquetOutputPath) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil  \n",
    "if os.path.exists('checkpoint/activity-data-append-memory/offsets'): \n",
    "    shutil.rmtree('checkpoint/activity-data-append-memory/offsets')     # 强制删除文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 09:41:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activityQuery3 = activityCounts.writeStream \\\n",
    "    .queryName(\"activity_query3\")\\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/activity-data-append-memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+-----+\n",
      "|user|window                                    |count|\n",
      "+----+------------------------------------------+-----+\n",
      "|c   |{2015-02-23 13:06:00, 2015-02-23 13:12:00}|13936|\n",
      "|h   |{2015-02-23 14:00:00, 2015-02-23 14:06:00}|14733|\n",
      "|g   |{2015-02-23 11:03:00, 2015-02-23 11:09:00}|16878|\n",
      "|b   |{2015-02-24 13:57:00, 2015-02-24 14:03:00}|18191|\n",
      "|i   |{2015-02-24 12:09:00, 2015-02-24 12:15:00}|14899|\n",
      "|g   |{2015-02-23 10:54:00, 2015-02-23 11:00:00}|23180|\n",
      "|g   |{2015-02-23 10:24:00, 2015-02-23 10:30:00}|16071|\n",
      "|g   |{2015-02-23 10:42:00, 2015-02-23 10:48:00}|13892|\n",
      "|e   |{2015-02-24 14:48:00, 2015-02-24 14:54:00}|23272|\n",
      "|d   |{2015-02-24 12:42:00, 2015-02-24 12:48:00}|16393|\n",
      "+----+------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from activity_query3\").show(10,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
