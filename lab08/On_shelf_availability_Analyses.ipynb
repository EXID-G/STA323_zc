{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab08 Case Study for Spark Structured APIs\n",
    "\n",
    "| Source from [Databricks Github on-shelf-availability](https://github.com/databricks-industry-solutions/on-shelf-availability)\n",
    "\n",
    "The definition of [on-shelf-availability](https://help.inspector-cloud.com/en/docs/business/kpi/osa/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7e91360-1e5f-4519-88fd-cf5f614939ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 1: Access Data\n",
    "\n",
    "We will identify potential out-of-stock and on-shelf availability issues requiring further scrutiny through the analysis of store inventory records:\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/osa_tredence_alerts.jpg' width=75%>\n",
    "\n",
    "Out-of-stock (OOS) scenarios occur when a retailer does not have enough inventory to meet consumer demand.  When an insufficient number of product units are made available to customers, not only are immediate sales lost but consumer confidence in the retailer is eroded. Out-of-stocks occur for a variety of reasons. Poor forecasting, limited supply, and operational challenges are all common causes. With each, swift action is required to identify and address the source of the problem less they continue to impact sales.  The challenge with out of stocks is that by the time it is identified, the lead time for requesting replacement units and making them available on the shelf for the consumer may require the retailer to live with the issue for quite some time. It is therefore important that any analysis of stocking levels consider the time to replenishment associated with a given item and location.\n",
    "\n",
    "A bit different from OOS issues are on-shelf availability (OSA) problems where inventory may be in the store but it's not placed in a manner that makes it easily accessible to customers. Product may be in inventory but the principal display may give the impression the item is out of stock or in low quantity.  Items may be on the shelf but not pulled forward in a manner that makes them easily viewable by customers.  Product may be technically in inventory but in a backroom that's not accessible to customers. Regardless of the reason, OSA issues tend to lead to lost revenue for retailers.\n",
    "\n",
    "To illustrate how analysis of OOS and OSA issues may be performed, Tredence has made available a simulated set of inventory and vendor data available for download [here](https://github.com/tredenceofficial/OSA-Data). To make these data available for use with this and the related notebooks, download the CSV files and then load them to your cloud storage environment.  \n",
    "\n",
    "We have automated this downloading step for you and used a */tmp/osa* storage path instead through out this accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/10 07:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "  .config(\"spark.log.level\", \"ERROR\") \\\n",
    "  .appName(\"lab8_exercise\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854eacd7-9d88-4e4d-81c1-e6ad8d6353da",
     "showTitle": true,
     "title": "Access the Inventory Data"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|inventory_pipeline|units_in_transit|units_in_dc|units_on_order|units_under_promotion|shelf_capacity|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|2021-04-12|     381| 64|     Category 01|                0|                      7|                  0|                 7|               0|          0|             0|                    0|            18|\n",
      "|2020-06-06|     531| 64|     Category 01|                0|                      0|                  0|                 0|               0|          0|             0|                    0|            24|\n",
      "|2019-01-30|     425| 64|     Category 01|               27|                     70|                 72|               238|              72|          0|            96|                    0|           100|\n",
      "|2019-02-15|    1452| 64|     Category 01|               44|                    225|                  0|               219|               0|          0|             0|                    0|           100|\n",
      "|2020-11-11|     338| 64|     Category 01|                0|                    187|                  0|               187|               0|          0|             0|                    0|           144|\n",
      "|2020-10-23|    1208|107|     Category 10|                0|                      3|                  0|                 3|               0|          0|             0|                    0|             5|\n",
      "|2019-01-23|    1936|153|     Category 02|                1|                     12|                  0|                12|               0|          0|             0|                    0|            36|\n",
      "|2020-01-28|    1392| 64|     Category 01|                4|                     15|                  0|                25|               0|          0|            12|                    0|            24|\n",
      "|2020-10-20|     790| 64|     Category 01|                0|                     15|                  0|               168|               0|          0|           156|                    0|            72|\n",
      "|2020-09-27|    1311|110|     Category 02|                0|                      5|                  0|                 5|               0|          0|             0|                    0|             0|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# schema for inventory data\n",
    "inventory_schema = StructType([\n",
    "  StructField('date',DateType()),\n",
    "  StructField('store_id',IntegerType()),\n",
    "  StructField('sku',IntegerType()),\n",
    "  StructField('product_category',StringType()),\n",
    "  StructField('total_sales_units',IntegerType()),\n",
    "  StructField('on_hand_inventory_units',IntegerType()),\n",
    "  StructField('replenishment_units',IntegerType()),\n",
    "  StructField('inventory_pipeline',IntegerType()),\n",
    "  StructField('units_in_transit',IntegerType()),\n",
    "  StructField('units_in_dc',IntegerType()),\n",
    "  StructField('units_on_order',IntegerType()),\n",
    "  StructField('units_under_promotion',IntegerType()),\n",
    "  StructField('shelf_capacity',IntegerType())\n",
    "  ])\n",
    "\n",
    "# read inventory data \n",
    "df1 = spark.read.csv(\n",
    "       'osa_raw_data.csv',\n",
    "       header = True,\n",
    "       schema = inventory_schema,\n",
    "       dateFormat = 'yyyyMMdd'\n",
    "       ).repartition(3) \n",
    "df1.createOrReplaceTempView(\"inventory_raw\")\n",
    "spark.table('inventory_raw').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "504f567b-de07-4dc7-94be-1b690245e30d",
     "showTitle": true,
     "title": "Access the Vendor Data"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+--------+---+---------------+--------------------+------------------+\n",
      "|key|vendor_id|sub_vendor_id|store_id|sku|lead_time_in_dc|lead_time_in_transit|lead_time_on_order|\n",
      "+---+---------+-------------+--------+---+---------------+--------------------+------------------+\n",
      "|  1|        1|         1001|    1763|  1|              4|                   3|                 7|\n",
      "|  2|        1|         1001|    1763|  2|              2|                   1|                 7|\n",
      "|  3|        1|         1002|    1843|  2|              2|                   1|                 7|\n",
      "|  4|        1|         1001|    1763|  3|              4|                   3|                 7|\n",
      "|  5|        2|         2016|     486|  6|              2|                   1|                 8|\n",
      "|  6|        2|         2073|    1587|  7|              3|                   2|                 9|\n",
      "|  7|        2|         2087|    1556|  8|              4|                   3|                 7|\n",
      "|  8|        8|         8010|    1283| 39|              3|                   2|                 6|\n",
      "|  9|       10|        10001|    1763| 46|              2|                   1|                 5|\n",
      "| 10|       11|        11001|     334| 52|              4|                   3|                 8|\n",
      "+---+---------+-------------+--------+---+---------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema for vendor data\n",
    "vendor_schema = StructType([\n",
    "  StructField('key',IntegerType()),\n",
    "  StructField('vendor_id',IntegerType()),\n",
    "  StructField('sub_vendor_id',IntegerType()),\n",
    "  StructField('store_id',IntegerType()),\n",
    "  StructField('item_id',IntegerType()),\n",
    "  StructField('lead_time_in_dc',IntegerType()),\n",
    "  StructField('lead_time_in_transit',IntegerType()),\n",
    "  StructField('lead_time_on_order',IntegerType()),\n",
    "])\n",
    "\n",
    "# read vendor data and persist as delta table\n",
    "df2 = spark.read.csv(\n",
    "     'vendor_leadtime_info.csv',\n",
    "     header = True,\n",
    "     schema = vendor_schema\n",
    "     ).withColumnRenamed('item_id','sku') # rename item_id to sku for consistency with inventory data\n",
    "   \n",
    "df2.createOrReplaceTempView(\"osa_vendor\")\n",
    "\n",
    "# review data\n",
    "spark.table('osa_vendor').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c9bf61-f077-4395-b922-ba2a7dba8b0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 2: Address Missing Records\n",
    "\n",
    "The inventory data contains records for products in specific stores when an inventory-related transaction occurs. Since not every product *moves* on every date, there will be days for which there is no data for certain store and product SKU combinations. \n",
    "\n",
    "Time series analysis techniques used in our framework require a complete set of records for products within a given location. To address the *missing* entries, we will generate a list of all dates for which we expect records. A cross-join with store-SKU combinations will provide the base set of records for which we expect data.  \n",
    "\n",
    "In the real world, not all products are intended to be sold at each location at all times.  In an analysis of non-simulated data, we may require additional information to determine the complete set of dates for a given store-SKU combination for which we should have data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bd3b75-4030-4e49-8814-90a1249e75bb",
     "showTitle": true,
     "title": "Assemble Complete Set of Dates"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2019-01-01|\n",
      "|2019-01-02|\n",
      "|2019-01-03|\n",
      "|2019-01-04|\n",
      "|2019-01-05|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate start and end of inventory dataset\n",
    "start_date, end_date = (\n",
    "  spark.table('inventory_raw').groupBy()\n",
    "    .agg(\n",
    "      f.min('date').alias('start_date'),\n",
    "      f.max('date').alias('end_date')  \n",
    "        )\n",
    "  .collect()[0]\n",
    "  )\n",
    "\n",
    "# generate contiguous set of dates within start and end range\n",
    "dates = (\n",
    "  spark\n",
    "    .range( (end_date - start_date).days + 1 )  # days in range\n",
    "    .withColumn('id', f.expr('cast(id as integer)')) # range value from long (bigint) to integer\n",
    "    .withColumn('date', f.lit(start_date) + f.col('id'))  # add range value to start date to generate contiguous date range\n",
    "    .select('date')\n",
    "  )\n",
    "\n",
    "# display dates\n",
    "dates.orderBy('date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f92ae98-7531-4acd-90a4-9854f95270f9",
     "showTitle": true,
     "title": "Assemble Complete Set of Stores-SKUs"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------------+\n",
      "|store_id|sku|product_category|\n",
      "+--------+---+----------------+\n",
      "|      63| 57|     Category 04|\n",
      "|      98| 64|     Category 01|\n",
      "|     164| 76|     Category 05|\n",
      "|     171| 64|     Category 01|\n",
      "|     178| 64|     Category 01|\n",
      "+--------+---+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract unique store-sku combinations in inventory records\n",
    "store_skus = (\n",
    "  spark\n",
    "    .table('inventory_raw')\n",
    "    .select('store_id','sku','product_category')\n",
    "    .groupBy('store_id','sku')\n",
    "      .agg(f.last('product_category').alias('product_category')) # just a hack to get last category assigned to each store-sku combination\n",
    "  )\n",
    "\n",
    "store_skus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab7e2c1-7422-44a5-8d71-74fff6e2384a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now cross-join the contiguous dates with each unique store-SKU found in the inventory dataset to create the expected records in our complete dataset.  Left outer joining these data to our actual inventory data, we will now have a complete set of records though there will be missing values in many fields which we will need to address in our next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26da6ed3-f9e0-4efd-bbd2-8813114e2551",
     "showTitle": true,
     "title": "Generate Complete Set of Inventory Records"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|inventory_pipeline|units_in_transit|units_in_dc|units_on_order|units_under_promotion|shelf_capacity|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|2019-01-01|      63| 57|     Category 04|             NULL|                   NULL|               NULL|              NULL|            NULL|       NULL|          NULL|                 NULL|          NULL|\n",
      "|2019-01-02|      63| 57|     Category 04|                0|                      8|                  0|                16|               0|          0|             8|                    0|            32|\n",
      "|2019-01-03|      63| 57|     Category 04|             NULL|                   NULL|               NULL|              NULL|            NULL|       NULL|          NULL|                 NULL|          NULL|\n",
      "|2019-01-04|      63| 57|     Category 04|                0|                      7|                  0|                14|               0|          8|             0|                    0|            32|\n",
      "|2019-01-05|      63| 57|     Category 04|                3|                      5|                  8|                12|               8|          0|             0|                    0|            32|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate one record for each store-sku for each date in range\n",
    "inventory_with_gaps = (\n",
    "  dates\n",
    "    .crossJoin(store_skus)\n",
    "    .join(\n",
    "      spark.table('inventory_raw').drop('product_category'), \n",
    "      on=['date','store_id','sku'], \n",
    "      how='leftouter'\n",
    "      )\n",
    "  )\n",
    "\n",
    "# display inventory records\n",
    "inventory_with_gaps.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517a3957-e7b0-42fd-947a-aaa370c8f2f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now have one record for each date-store-SKU combination in our dataset.  However, on those dates for which there were no inventory changes, we are currently missing information about the inventory status of those stores and SKUs.  To address this, we will employ a combination of forward filling, *i.e.* applying the last valid record to subsequent records until a new value is encountered, and defaults.  For the forward fill, we will make use of the [last()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.last.html) function, providing a value of *True* for the *ignorenulls* argument which will force it to retrieve the last non-null value in a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec5194a-34d3-4aa1-90e7-e842194302a8",
     "showTitle": true,
     "title": "Impute Missing Values"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|inventory_pipeline|units_in_transit|units_in_dc|units_on_order|units_under_promotion|shelf_capacity|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "|2019-01-01|      63| 57|     Category 04|                0|                   NULL|                  0|                 0|               0|          0|             0|                    0|          NULL|\n",
      "|2019-01-02|      63| 57|     Category 04|                0|                      8|                  0|                16|               0|          0|             8|                    0|            32|\n",
      "|2019-01-03|      63| 57|     Category 04|                0|                      8|                  0|                 0|               0|          0|             0|                    0|            32|\n",
      "|2019-01-04|      63| 57|     Category 04|                0|                      7|                  0|                14|               0|          8|             0|                    0|            32|\n",
      "|2019-01-05|      63| 57|     Category 04|                3|                      5|                  8|                12|               8|          0|             0|                    0|            32|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy dataframe to enable manipulations in loop\n",
    "inventory_cleansed = inventory_with_gaps\n",
    "\n",
    "# apply forward fill to appropriate columns\n",
    "for c in ['shelf_capacity', 'on_hand_inventory_units']:\n",
    "  inventory_cleansed = (\n",
    "    inventory_cleansed\n",
    "      .withColumn(\n",
    "          c, \n",
    "          f.expr('LAST({0}, True) OVER(PARTITION BY store_id, sku ORDER BY date)'.format(c)) # get last non-null prior value (aka forward-fill)\n",
    "           )\n",
    "        )\n",
    "  \n",
    "# apply default value of 0 to appropriate columns\n",
    "inventory_cleansed = (\n",
    "  inventory_cleansed\n",
    "    .fillna(\n",
    "      0, \n",
    "      [ 'total_sales_units',\n",
    "        'units_under_promotion',\n",
    "        'units_in_transit',\n",
    "        'units_in_dc',\n",
    "        'units_on_order',\n",
    "        'replenishment_units',\n",
    "        'inventory_pipeline'\n",
    "        ]\n",
    "      )\n",
    "  )\n",
    "\n",
    "# display data with imputed values\n",
    "inventory_cleansed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df3ad085-f7e6-4c76-9a44-51a1fd971c58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 3: Identify Key Inventory Events\n",
    "\n",
    "With our complete inventory dataset in-hand, we can now identify key inventory-related events within the data.  These include the occurrence of promotions intended to drive product sales and replenishment events during which new units are added to inventory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ce20e5-c6ac-4e5c-8d67-f3610b0cd583",
     "showTitle": true,
     "title": "Calculate Inventory Flags"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+--------------+------------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|inventory_pipeline|units_in_transit|units_in_dc|units_on_order|units_under_promotion|shelf_capacity|promotion_flag|replenishment_flag|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+--------------+------------------+\n",
      "|2019-01-01|      63| 57|     Category 04|                0|                   NULL|                  0|                 0|               0|          0|             0|                    0|          NULL|             0|                 0|\n",
      "|2019-01-02|      63| 57|     Category 04|                0|                      8|                  0|                16|               0|          0|             8|                    0|            32|             0|                 0|\n",
      "|2019-01-03|      63| 57|     Category 04|                0|                      8|                  0|                 0|               0|          0|             0|                    0|            32|             0|                 0|\n",
      "|2019-01-04|      63| 57|     Category 04|                0|                      7|                  0|                14|               0|          8|             0|                    0|            32|             0|                 0|\n",
      "|2019-01-05|      63| 57|     Category 04|                3|                      5|                  8|                12|               8|          0|             0|                    0|            32|             0|                 1|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+----------------+-----------+--------------+---------------------+--------------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# derive inventory flags\n",
    "inventory_final = (\n",
    "  inventory_cleansed\n",
    "    .withColumn('promotion_flag', f.expr('CASE WHEN units_under_promotion > 0 THEN 1 ELSE 0 END'))\n",
    "    .withColumn('replenishment_flag', f.expr('CASE WHEN replenishment_units > 0 THEN 1 ELSE 0 END'))\n",
    "    )\n",
    "\n",
    "inventory_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a2925e-65d3-497d-bd10-df2701b02a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now persist this data for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a934308-04aa-46ae-a038-f35206a790a7",
     "showTitle": true,
     "title": "Persist Updated Inventory Data"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.catalog.dropTempView('osa_inventory')\n",
    "\n",
    "# (\n",
    "#   inventory_final\n",
    "#     .repartition(3)\n",
    "#     .write\n",
    "#       .format('parquet')\n",
    "#       .mode('overwrite')\n",
    "#       .option('overwriteSchema', 'true')\n",
    "#       .saveAsTable('osa_inventory')\n",
    "#    )\n",
    "inventory_final.createOrReplaceTempView(\"osa_inventory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41de3f9e-dd3a-4025-8610-8e66c260773b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 4: Access Data\n",
    "\n",
    "Our first step is to access the inventory and vendor data assembled and cleansed in the last notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9076df9-088b-4918-bfff-e5ce150b5dca",
     "showTitle": true,
     "title": "Retrieve Data"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inventory = spark.table('osa_inventory')\n",
    "vendor = spark.table('osa_vendor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2310f543-cf81-4ef4-8a63-3c15aec911f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 5: Identify Phantom Inventory\n",
    "\n",
    "Next, we will identify problems related to phantom inventory.  Phantom inventory represents product units not accounted for between the inventory and the sales systems.  These units may represent items misplaced, stolen, lost or otherwise inaccurately tracked in one system or the other.  The identification of these units is essential for ensuring the right quantity, *i.e.* not too few and not too many, of a given product are purchased for replenishment and may point to needed operational improvements to ensure accurate inventories are maintained moving forward.\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/osa_tredence_phantominventory.png' width=75%>\n",
    "\n",
    "Phantom inventory is identified by simply calculating the number of units expected to be on-hand at the end of the day relative to those actually on-hand. Minor differences between the units expected and those actually in inventory may not require immediate attention.  A phantom inventory indicator flag is set when the phantom inventory is some multiple of the average daily sales for a given product. Here, we set this multiple to 5-times but some organizations may wish to be more or less sensitive to the detection of problematic levels of phantom inventory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfa9ee87-c801-4dda-927c-d5099cf8de19",
     "showTitle": true,
     "title": "Calculate Phantom Inventory"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+-----------------+-------------------+-------------------+-----------------+-----------------+-----------------+---------------------+\n",
      "|      date|store_id|sku|daily_sales_units|start_on_hand_units|replenishment_units|total_sales_units|end_on_hand_units|phantom_inventory|phantom_inventory_ind|\n",
      "+----------+--------+---+-----------------+-------------------+-------------------+-----------------+-----------------+-----------------+---------------------+\n",
      "|2019-01-01|      63| 57|              0.0|               NULL|                  0|                0|                0|             NULL|                    0|\n",
      "|2019-01-02|      63| 57|              0.0|                  8|                  0|                0|                8|                0|                    0|\n",
      "|2019-01-03|      63| 57|              0.0|                  8|                  0|                0|                8|                0|                    0|\n",
      "|2019-01-04|      63| 57|              0.0|                  8|                  0|                0|                7|                1|                    1|\n",
      "|2019-01-05|      63| 57|              0.6|                  7|                  8|                3|                5|                7|                    1|\n",
      "+----------+--------+---+-----------------+-------------------+-------------------+-----------------+-----------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# phantom inventory calculations\n",
    "phantom_inventory = (\n",
    "  inventory\n",
    "    \n",
    "    # average daily sales\n",
    "    .withColumn('daily_sales_units', f.expr('AVG(total_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date)')) \n",
    "    \n",
    "    # on-hand inventory units at the end of previous day\n",
    "    # for dates with no prior day inventory units, provide alt calculation\n",
    "    .withColumn('start_on_hand_units', f.expr('''\n",
    "      COALESCE( \n",
    "        LAG(on_hand_inventory_units, 1) OVER(PARTITION BY store_id, sku ORDER BY date), \n",
    "        on_hand_inventory_units + total_sales_units - replenishment_units\n",
    "        )\n",
    "        ''')) \n",
    "    \n",
    "    # on-hand inventory units at end of day\n",
    "    .withColumn('end_on_hand_units', f.expr('COALESCE(on_hand_inventory_units, 0)')) \n",
    "    \n",
    "    # calculate phantom inventory as difference in:\n",
    "    # (previous day's on-hand inventory + current day's replenished units - current day's sales units) and current day's end-of-day inventory \n",
    "    .withColumn('phantom_inventory', f.expr('start_on_hand_units + replenishment_units - total_sales_units - end_on_hand_units')) \n",
    "    \n",
    "    # flag only when phantom inventory is at least 5 times average daily sales\n",
    "    .withColumn('phantom_inventory_ind', f.expr('''\n",
    "      CASE\n",
    "        WHEN phantom_inventory <> 0 AND ABS(phantom_inventory) > 5 * daily_sales_units THEN 1 \n",
    "        ELSE 0 \n",
    "        END'''))  \n",
    "  \n",
    "    .select(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'daily_sales_units',\n",
    "      'start_on_hand_units',\n",
    "      'replenishment_units',\n",
    "      'total_sales_units',\n",
    "      'end_on_hand_units',\n",
    "      'phantom_inventory',\n",
    "      'phantom_inventory_ind'\n",
    "      )\n",
    "    )\n",
    "\n",
    "# display results\n",
    "phantom_inventory.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aa618eb-6a42-4ddd-9352-0774af853a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 6: Identify Out of Stocks\n",
    "\n",
    "Out of stocks occur when inventory is not sufficient to meet demand.  Most retailers define a safety stock level that serves as the threshold for triggering replenishment orders.  When inventory dips below the safety stock level, a replenishment order is generated.  The remaining inventory on-hand must then be sufficient to meet demand until the replacement units arrive.\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/osa_tredence_safetystock.png' width=75%>\n",
    "\n",
    "There are numerous ways to calculate a safety stock level for a given product.  Here, we will consider the store-specific dynamics associated with a product to arrive at two potentially valid safety stock levels.  The first of these is the number of units on-hand before past replenishment events. The second of these will consider average daily sales relative to lead times for a product. For a given store-SKU combination, we will take the lower of these two values to be our safety stock level.  But before we can calculate these values, we need to estimate the inventory on-hand, something we tackled in our prior phantom inventory calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b611a5-93d1-4cfa-a16f-9f1c9b1010b4",
     "showTitle": true,
     "title": "Join Inventory to Phantom Inventory"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+\n",
      "|store_id|sku|      date|product_category|on_hand_inventory_units|total_sales_units|replenishment_units|replenishment_flag|units_on_order|units_in_transit|units_in_dc|phantom_inventory|estimated_on_hand_inventory|\n",
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+\n",
      "|      63| 57|2019-05-11|     Category 04|                      1|                0|                  0|                 0|             0|               0|          0|                0|                          0|\n",
      "|      63| 57|2019-09-12|     Category 04|                     23|                0|                  0|                 0|             0|               0|          0|                0|                          0|\n",
      "|      63| 57|2020-02-24|     Category 04|                     23|                3|                  0|                 0|             0|               0|          0|               -1|                          0|\n",
      "|      63| 57|2020-10-25|     Category 04|                     35|                2|                  0|                 0|            24|               0|          0|              -37|                          0|\n",
      "|      98| 64|2019-04-20|     Category 01|                     21|                8|                  0|                 0|             0|               0|          0|              -18|                          0|\n",
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine inventory with phantom inventory and min lead times\n",
    "inventory_with_pi = (\n",
    "  inventory.alias('inv')\n",
    "    .join(phantom_inventory.alias('pi'), on=['store_id','sku','date'])\n",
    "  \n",
    "    # limit fields to use moving forward\n",
    "    .selectExpr(\n",
    "      'inv.store_id',\n",
    "      'inv.sku',\n",
    "      'inv.date',\n",
    "      'inv.product_category',\n",
    "      'inv.on_hand_inventory_units',\n",
    "      'inv.total_sales_units',\n",
    "      'inv.replenishment_units',\n",
    "      'inv.replenishment_flag',\n",
    "      'inv.units_on_order',\n",
    "      'inv.units_in_transit',\n",
    "      'inv.units_in_dc',\n",
    "      'pi.phantom_inventory'\n",
    "      )\n",
    "  \n",
    "  # correct inventory values to enable calculations\n",
    "  .withColumn('phantom_inventory', f.expr('COALESCE(phantom_inventory, 0)')) \n",
    "  .withColumn('on_hand_inventory_units', f.expr('''\n",
    "              CASE \n",
    "                WHEN on_hand_inventory_units < 0 THEN 0 \n",
    "                ELSE on_hand_inventory_units \n",
    "                END''')\n",
    "             )\n",
    "   .withColumn('replenishment_units', f.expr('''\n",
    "              CASE \n",
    "                WHEN replenishment_flag = 1 THEN replenishment_units\n",
    "                ELSE 0 \n",
    "                END''')\n",
    "             )\n",
    "  \n",
    "  # initialize estimated on-hand inventory field\n",
    "   .withColumn('estimated_on_hand_inventory', f.lit(0)) \n",
    "  )\n",
    "\n",
    "inventory_with_pi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f5a9ec-f49b-4158-8819-30c3dae56661",
     "showTitle": true,
     "title": "Estimate On-Hand Inventory"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 4.0.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py:53\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     have_arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inventory_pd\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# calculate estimated on-hand inventory\u001b[39;00m\n\u001b[1;32m     33\u001b[0m inventory_on_hand \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     34\u001b[0m   inventory_with_pi\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msku\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;241m.\u001b[39mapplyInPandas( get_estimated_inventory, schema\u001b[38;5;241m=\u001b[39minventory_with_pi\u001b[38;5;241m.\u001b[39mschema )\n\u001b[1;32m     37\u001b[0m   )\n\u001b[1;32m     39\u001b[0m inventory_on_hand\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/group_ops.py:228\u001b[0m, in \u001b[0;36mPandasGroupedOpsMixin.applyInPandas\u001b[0;34m(self, func, schema)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas_udf, PandasUDFType\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, GroupedData)\n\u001b[0;32m--> 228\u001b[0m udf \u001b[38;5;241m=\u001b[39m pandas_udf(func, returnType\u001b[38;5;241m=\u001b[39mschema, functionType\u001b[38;5;241m=\u001b[39mPandasUDFType\u001b[38;5;241m.\u001b[39mGROUPED_MAP)\n\u001b[1;32m    229\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n\u001b[1;32m    230\u001b[0m udf_column \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;241m*\u001b[39m[df[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns])\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/functions.py:338\u001b[0m, in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# are not yet visible to the user. Some of behaviors are buggy and might be changed in the near\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# future. The table might have to be eventually documented externally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Note: Timezone is KST.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Note: 'X' means it throws an exception during the conversion.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m--> 338\u001b[0m require_minimum_pyarrow_version()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# decorator @pandas_udf(returnType, functionType)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m is_decorator \u001b[38;5;241m=\u001b[39m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, (\u001b[38;5;28mstr\u001b[39m, DataType))\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py:60\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_arrow:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pyarrow_version\n\u001b[1;32m     63\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pyarrow\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pyarrow_version):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pyarrow_version, pyarrow\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     68\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: PyArrow >= 4.0.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# iterate over inventory to calculate current inventory levels\n",
    "def get_estimated_inventory(inventory_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "  \n",
    "  inventory_pd.sort_values('date', inplace=True)\n",
    "\n",
    "  # iterate over records in inventory data\n",
    "  for i in range(1,len(inventory_pd)):\n",
    "    \n",
    "    # get component values\n",
    "    previous_inv = inventory_pd.estimated_on_hand_inventory.loc[i-1].copy()\n",
    "    if previous_inv < 0: \n",
    "        previous_inv = 0\n",
    "    \n",
    "    replenishment_units = inventory_pd.replenishment_units.loc[i].copy()\n",
    "    total_sales_units = inventory_pd.total_sales_units.loc[i].copy()\n",
    "    phantom_inventory_units = inventory_pd.phantom_inventory.loc[i].copy()\n",
    "    on_hand_inventory_units = inventory_pd.on_hand_inventory_units.loc[i].copy()\n",
    "    \n",
    "    # calculate estimated on-hand inventory\n",
    "    estimated_on_hand_inventory = (previous_inv + replenishment_units - total_sales_units - phantom_inventory_units)\n",
    "    if estimated_on_hand_inventory < 0: estimated_on_hand_inventory = 0\n",
    "    if estimated_on_hand_inventory > on_hand_inventory_units: estimated_on_hand_inventory = on_hand_inventory_units\n",
    "    \n",
    "    with warnings.catch_warnings(record=True):\n",
    "        inventory_pd.estimated_on_hand_inventory.loc[i] = estimated_on_hand_inventory\n",
    "\n",
    "  return inventory_pd\n",
    "\n",
    "\n",
    "# calculate estimated on-hand inventory\n",
    "inventory_on_hand = (\n",
    "  inventory_with_pi\n",
    "  .groupby('store_id', 'sku')\n",
    "    .applyInPandas( get_estimated_inventory, schema=inventory_with_pi.schema )\n",
    "  )\n",
    "\n",
    "inventory_on_hand.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e57bfd61-f29d-45a8-93d0-1387a08548fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using this data, we can now calculate the average amount on-hand prior to a replenishment event as well as the average number of units sold for each store-SKU.  For both metrics, we'll limit the calculation to the 90 days prior to the current inventory record.  This will allow for changes in stocking practices and sales velocity over the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060f2cf7-86f3-4800-a0d2-5106b1a1a443",
     "showTitle": true,
     "title": "Calculate Average On-Hand and Average Daily Sales Metrics"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+---------------+--------------------+--------------------------+------------------+------------------+\n",
      "|store_id|sku|      date|product_category|on_hand_inventory_units|total_sales_units|replenishment_units|replenishment_flag|units_on_order|units_in_transit|units_in_dc|phantom_inventory|estimated_on_hand_inventory|prior_inventory|rolling_stock_onhand|rolling_min_expected_stock|min_expected_stock| daily_sales_units|\n",
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+---------------+--------------------+--------------------------+------------------+------------------+\n",
      "|      63| 57|2019-01-01|     Category 04|                   NULL|                0|                  0|                 0|             0|               0|          0|                0|                          0|              0|                 0.0|                       0.0|               0.0|               0.0|\n",
      "|      63| 57|2019-01-02|     Category 04|                      8|                0|                  0|                 0|             8|               0|          0|                0|                          1|              0|                 0.0|                       0.0|               0.0|               0.0|\n",
      "|      63| 57|2019-01-03|     Category 04|                      8|                0|                  0|                 0|             0|               0|          0|                0|                          8|              0|                 0.0|                       0.0|               0.0|               0.0|\n",
      "|      63| 57|2019-01-04|     Category 04|                      7|                0|                  0|                 0|             0|               0|          8|                1|                          7|              0|                 0.0|                       0.0|               0.0|               0.0|\n",
      "|      63| 57|2019-01-05|     Category 04|                      5|                3|                  8|                 1|             0|               8|          0|                7|                          0|              7|                 3.5|                       3.5|               3.5|               0.6|\n",
      "|      63| 57|2019-01-06|     Category 04|                     11|                2|                  0|                 0|             0|               0|          0|               -8|                          7|              0|                 3.5|                       0.0|               3.5|0.8333333333333334|\n",
      "|      63| 57|2019-01-07|     Category 04|                     11|                0|                  0|                 0|             0|               0|          0|                0|                          0|              0|                 3.5|                       0.0|               3.5|0.7142857142857143|\n",
      "|      63| 57|2019-01-08|     Category 04|                     11|                0|                  0|                 0|             0|               0|          0|                0|                          0|              0|                 3.5|                       0.0|               3.5|             0.625|\n",
      "|      63| 57|2019-01-09|     Category 04|                      7|                3|                  0|                 0|             8|               0|          0|                1|                          0|              0|                 3.5|                       0.0|               3.5|0.8888888888888888|\n",
      "|      63| 57|2019-01-10|     Category 04|                      5|                2|                  0|                 0|             8|               0|          0|                0|                          0|              0|                 3.5|                       0.0|               3.5|               1.0|\n",
      "+--------+---+----------+----------------+-----------------------+-----------------+-------------------+------------------+--------------+----------------+-----------+-----------------+---------------------------+---------------+--------------------+--------------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inventory_with_metrics = (\n",
    "  inventory_on_hand\n",
    "    \n",
    "    # AVERAGE ON-HAND UNITS PRIOR TO REPLENISHMENT\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # getting prior day's on-hand inventory only for the days with replenishment\n",
    "    .withColumn('prior_inventory', f.expr('LAG(estimated_on_hand_inventory, 1) OVER(PARTITION BY store_id, sku ORDER BY date)'))\n",
    "    .withColumn('prior_inventory', f.expr('COALESCE(prior_inventory,0)'))\n",
    "    .withColumn('prior_inventory', f.expr('CASE WHEN replenishment_flag=1 THEN prior_inventory ELSE 0 END'))\n",
    "  \n",
    "    # calculating rolling average of prior day's on-hand inventory for days with replenishment (over last 90 days)\n",
    "    .withColumn('rolling_stock_onhand', f.expr('''\n",
    "      SUM(prior_inventory) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 90 PRECEDING AND CURRENT ROW) /\n",
    "      (SUM(replenishment_flag) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 90 PRECEDING AND CURRENT ROW) + 1)\n",
    "      '''\n",
    "      ))\n",
    "    .withColumn('rolling_min_expected_stock', f.expr('CASE WHEN replenishment_flag != 1 THEN 0 ELSE rolling_stock_onhand END'))\n",
    "    .withColumn('rolling_min_expected_stock', f.expr('COALESCE(rolling_min_expected_stock,0)'))\n",
    "    \n",
    "    # fixing the inventory values for all dates through forward fill\n",
    "    .withColumn('min_expected_stock', f.expr('NULLIF(rolling_min_expected_stock,0)'))\n",
    "    .withColumn('min_expected_stock', f.expr('LAST(min_expected_stock, True) OVER(PARTITION BY store_id, sku ORDER BY date)'))\n",
    "    .withColumn('min_expected_stock', f.expr('COALESCE(min_expected_stock, 0)'))\n",
    "    # ------------------------------------------------------------------------------------\n",
    "  \n",
    "    # AVERAGE DAILY SALES\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # getting daily sales velocity\n",
    "    .withColumn('daily_sales_units', f.expr('AVG(total_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 90 PRECEDING AND CURRENT ROW)'))\n",
    "    .withColumn('daily_sales_units', f.expr('LAST(daily_sales_units, True) OVER(PARTITION BY store_id, sku ORDER BY date)'))\n",
    "    .withColumn('daily_sales_units', f.expr('COALESCE(daily_sales_units, 0)'))\n",
    "    # ------------------------------------------------------------------------------------\n",
    "  )\n",
    "\n",
    "inventory_with_metrics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01af4e6c-0013-4f75-b8de-568af31f6bc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To derive safety stock levels using average daily sales, we need to know something about the lead times for the replenishment of particular SKUs within a given store location.  We can derive this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6725409a-ab26-4567-8d2e-b9d05b703515",
     "showTitle": true,
     "title": "Calculate Shortest Lead Time for Each Store-SKU"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------+---------------+--------------------+------------------+\n",
      "|store_id|sku|min_lead_time|lead_time_in_dc|lead_time_in_transit|lead_time_on_order|\n",
      "+--------+---+-------------+---------------+--------------------+------------------+\n",
      "|    1763|  1|            3|              4|                   3|                 7|\n",
      "|    1763|  2|            1|              2|                   1|                 7|\n",
      "|    1843|  2|            1|              2|                   1|                 7|\n",
      "|    1763|  3|            3|              4|                   3|                 7|\n",
      "|     486|  6|            1|              2|                   1|                 8|\n",
      "+--------+---+-------------+---------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate shortest lead time for each store-sku combination\n",
    "lead_time = (\n",
    "  vendor\n",
    "    .withColumn('min_lead_time', f.expr('LEAST(lead_time_in_dc, lead_time_in_transit, lead_time_on_order)'))\n",
    "    .select('store_id', 'sku', 'min_lead_time', 'lead_time_in_dc', 'lead_time_in_transit', 'lead_time_on_order')\n",
    "    )\n",
    "\n",
    "lead_time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd203e2e-d7f7-48df-8d4e-937a07b38a09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can now calculate the safety stock requirements using the lower of the two values calculated from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54626b63-ff71-4a2f-90fb-7ddc689c42dd",
     "showTitle": true,
     "title": "Determine Safety Stock"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|replenishment_flag|phantom_inventory|estimated_on_hand_inventory|prior_inventory|rolling_min_expected_stock|min_expected_stock|daily_sales_units|      safety_stock|units_on_order|units_in_transit|units_in_dc|lead_time_in_transit|lead_time_in_dc|lead_time_on_order|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+\n",
      "|2019-01-01|      63| 57|     Category 04|                0|                   NULL|                  0|                 0|                0|                          0|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          0|                   3|              4|                10|\n",
      "|2019-01-02|      63| 57|     Category 04|                0|                      8|                  0|                 0|                0|                          1|              0|                       0.0|               0.0|              0.0|               0.0|             8|               0|          0|                   3|              4|                10|\n",
      "|2019-01-03|      63| 57|     Category 04|                0|                      8|                  0|                 0|                0|                          8|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          0|                   3|              4|                10|\n",
      "|2019-01-04|      63| 57|     Category 04|                0|                      7|                  0|                 0|                1|                          7|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          8|                   3|              4|                10|\n",
      "|2019-01-05|      63| 57|     Category 04|                3|                      5|                  8|                 1|                7|                          0|              7|                       3.5|               3.5|              0.6|1.7999999999999998|             0|               8|          0|                   3|              4|                10|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inventory_safety_stock = (\n",
    "  inventory_with_metrics\n",
    "    .join(lead_time, on=['store_id','sku'], how='leftouter')\n",
    "  \n",
    "    # safety stock for sales velocity is avg daily sales units * min_lead_time\n",
    "    .withColumn('ss_sales_velocity', f.expr('daily_sales_units * min_lead_time'))\n",
    "  \n",
    "    # use the lower of the min_expected_stock at replinishment or sales_velocity-derived stock requirement as safety stock\n",
    "    .withColumn('safety_stock', f.expr('CASE WHEN min_expected_stock < ss_sales_velocity THEN min_expected_stock ELSE ss_sales_velocity END'))\n",
    "    .withColumn('safety_stock', f.expr('CASE WHEN replenishment_flag != 1 THEN 0 ELSE safety_stock END'))\n",
    "    .withColumn('safety_stock', f.expr('COALESCE(safety_stock,0)'))\n",
    "    .withColumn('safety_stock', f.expr('CASE WHEN safety_stock=0 THEN min_expected_stock ELSE safety_stock END'))\n",
    "  \n",
    "    .select(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'product_category',\n",
    "      'total_sales_units', \n",
    "      'on_hand_inventory_units',\n",
    "      'replenishment_units', \n",
    "      'replenishment_flag',\n",
    "      'phantom_inventory',\n",
    "      'estimated_on_hand_inventory',  \n",
    "      'prior_inventory',\n",
    "      'rolling_min_expected_stock', \n",
    "      'min_expected_stock',\n",
    "      'daily_sales_units', \n",
    "      'safety_stock', \n",
    "      'units_on_order',\n",
    "      'units_in_transit',\n",
    "      'units_in_dc',\n",
    "      'lead_time_in_transit',\n",
    "      'lead_time_in_dc',\n",
    "      'lead_time_on_order'\n",
    "      )\n",
    "  )\n",
    "\n",
    "inventory_safety_stock.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc76009-6a4d-4da7-9ae0-a384d96fbaaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With a safety stock level defined for each store-SKU, we can now identify dates where:</p>\n",
    "\n",
    "1. The on-hand inventory is less than the safety stock level (*on_hand_less_than_safety_stock*)\n",
    "2. The requested replinishment units are not sufficient to meet safety stock requirements (*insufficient_inventory_pipeline_units*)\n",
    "3. The inventory pipeline is one day away from not being able to fulfill stocking requirements (*insufficient_lead_time*)\n",
    "\n",
    "Each of these conditions represents an inventory management problem which requires addressing. The first two of these conditions may be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa23672-d2cb-4f46-8cf3-ec4c21b5c6da",
     "showTitle": true,
     "title": "Identify Insufficient Inventory On-Hand & In-Pipeline Events"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 226:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|replenishment_flag|phantom_inventory|estimated_on_hand_inventory|prior_inventory|rolling_min_expected_stock|min_expected_stock|daily_sales_units|      safety_stock|units_on_order|units_in_transit|units_in_dc|lead_time_in_transit|lead_time_in_dc|lead_time_on_order|on_hand_less_than_safety_stock|insufficient_inventory_pipeline_units|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "|2019-01-01|      63| 57|     Category 04|                0|                   NULL|                  0|                 0|                0|                          0|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "|2019-01-02|      63| 57|     Category 04|                0|                      8|                  0|                 0|                0|                          1|              0|                       0.0|               0.0|              0.0|               0.0|             8|               0|          0|                   3|              4|                10|                             0|                                    0|\n",
      "|2019-01-03|      63| 57|     Category 04|                0|                      8|                  0|                 0|                0|                          8|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          0|                   3|              4|                10|                             0|                                    0|\n",
      "|2019-01-04|      63| 57|     Category 04|                0|                      7|                  0|                 0|                1|                          7|              0|                       0.0|               0.0|              0.0|               0.0|             0|               0|          8|                   3|              4|                10|                             0|                                    0|\n",
      "|2019-01-05|      63| 57|     Category 04|                3|                      5|                  8|                 1|                7|                          0|              7|                       3.5|               3.5|              0.6|1.7999999999999998|             0|               8|          0|                   3|              4|                10|                             1|                                    0|\n",
      "+----------+--------+---+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-----------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inventory_safety_stock_alert = (\n",
    "  inventory_safety_stock\n",
    "  \n",
    "  # alert 1 - estimated on-hand inventory is less than safety stock\n",
    "  .withColumn('on_hand_less_than_safety_stock', f.expr('CASE WHEN estimated_on_hand_inventory <= safety_stock THEN 1 ELSE 0 END'))\n",
    "  \n",
    "  # alert 2 - inventory in pipeline is not sufficient to reach the safety stock levels\n",
    "  .withColumn('insufficient_inventory_pipeline_units', f.expr('''\n",
    "    CASE\n",
    "      WHEN  (on_hand_less_than_safety_stock = 1) AND \n",
    "            (units_on_order + units_in_transit + units_in_dc != 0) AND\n",
    "            ((units_on_order + units_in_transit + units_in_dc) < (safety_stock - estimated_on_hand_inventory))\n",
    "         THEN 1\n",
    "      ELSE 0\n",
    "      END'''))\n",
    "  )\n",
    "\n",
    "inventory_safety_stock_alert.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58311de5-585c-485b-83ac-c4c91b118613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can identify dates where there was insufficient lead time in the pipeline to meet expected demand, the last of our three events, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee4cdbf-5128-4ff7-bfa0-0cbb1e225284",
     "showTitle": true,
     "title": "Identify Insufficient Lead Time Events"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+----------------------+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-------------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "|store_id|sku|      date|insufficient_lead_time|product_category|total_sales_units|on_hand_inventory_units|replenishment_units|replenishment_flag|phantom_inventory|estimated_on_hand_inventory|prior_inventory|rolling_min_expected_stock|min_expected_stock|  daily_sales_units|      safety_stock|units_on_order|units_in_transit|units_in_dc|lead_time_in_transit|lead_time_in_dc|lead_time_on_order|on_hand_less_than_safety_stock|insufficient_inventory_pipeline_units|\n",
      "+--------+---+----------+----------------------+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-------------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "|      63| 57|2019-05-11|                     0|     Category 04|                0|                      1|                  0|                 0|                0|                          0|              0|                       0.0|             0.625| 0.8681318681318682|             0.625|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "|      63| 57|2019-09-12|                     0|     Category 04|                0|                     23|                  0|                 0|                0|                          0|              0|                       0.0|               0.5| 0.7032967032967034|               0.5|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "|      63| 57|2020-02-24|                     0|     Category 04|                3|                     23|                  0|                 0|               -1|                          0|              0|                       0.0|             1.875| 0.9560439560439561|             1.875|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "|      63| 57|2020-10-25|                     0|     Category 04|                2|                     35|                  0|                 0|              -37|                         35|              0|                       0.0|0.3333333333333333| 0.4945054945054945|0.3333333333333333|            24|               0|          0|                   3|              4|                10|                             0|                                    0|\n",
      "|      98| 64|2019-04-20|                     1|     Category 01|                8|                     21|                  0|                 0|              -18|                          0|              0|                       0.0|               0.0| 1.4945054945054945|               0.0|             0|               0|          0|                   2|              3|                 9|                             1|                                    0|\n",
      "|      98| 64|2020-07-16|                     1|     Category 01|                0|                      0|                  0|                 0|                0|                          0|              0|                       0.0|             2.875|                0.0|             2.875|             0|               0|          0|                   2|              3|                 9|                             1|                                    0|\n",
      "|      98| 64|2020-09-23|                     1|     Category 01|                0|                      0|                  0|                 0|                0|                          0|              0|                       0.0|             2.875|                0.0|             2.875|             0|               0|          0|                   2|              3|                 9|                             1|                                    0|\n",
      "|      98| 64|2020-12-05|                     0|     Category 01|                0|                     13|                  0|                 0|                0|                          0|              0|                       0.0|             2.875| 0.9120879120879121|             2.875|             0|               0|          0|                   2|              3|                 9|                             1|                                    0|\n",
      "|     164| 76|2019-04-06|                     0|     Category 05|                0|                      7|                  0|                 0|                0|                          0|              0|                       0.0|0.2857142857142857|0.23076923076923078|0.2857142857142857|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "|     164| 76|2019-05-19|                     1|     Category 05|                0|                     10|                  0|                 0|                0|                          0|              0|                       0.0|0.2857142857142857| 0.3076923076923077|0.2857142857142857|             0|               0|          0|                   3|              4|                10|                             1|                                    0|\n",
      "+--------+---+----------+----------------------+----------------+-----------------+-----------------------+-------------------+------------------+-----------------+---------------------------+---------------+--------------------------+------------------+-------------------+------------------+--------------+----------------+-----------+--------------------+---------------+------------------+------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate lead times associated with inventory records\n",
    "inventory_safety_stock_with_lead_times = (\n",
    "  \n",
    "  inventory_safety_stock_alert\n",
    "  \n",
    "    # lead time values at store-sku level for various stages \n",
    "    .withColumn('lead_time_in_transit', f.expr('COALESCE(lead_time_in_transit,0)'))\n",
    "    .withColumn('lead_time_on_order', f.expr('COALESCE(lead_time_on_order,0)'))\n",
    "    .withColumn('lead_time_in_dc', f.expr('COALESCE(lead_time_in_dc,0)'))\n",
    "\n",
    "    # considering lead time only if estimated on-hand inventory and inventory in pipeline meet the safety stock levels\n",
    "    .withColumn('lead_time', f.expr('''\n",
    "       CASE\n",
    "         WHEN on_hand_less_than_safety_stock = 1 AND\n",
    "              (units_on_order + units_in_transit + units_in_dc != 0) AND\n",
    "              ((units_on_order + units_in_transit + units_in_dc) >= (safety_stock - estimated_on_hand_inventory)) \n",
    "           THEN GREATEST(\n",
    "                 COALESCE(lead_time_in_transit,0),\n",
    "                 COALESCE(lead_time_on_order,0),\n",
    "                 COALESCE(lead_time_in_dc,0)\n",
    "                 )+1\n",
    "         ELSE null \n",
    "         END'''))\n",
    "  )\n",
    "\n",
    "# identify lead time problems\n",
    "lead_time_alerts = (\n",
    "  \n",
    "  inventory_safety_stock_with_lead_times.alias('a')\n",
    "  \n",
    "    # self join to get the previous lead time (most recent one) for the inventory pipeline\n",
    "    .join(\n",
    "      (inventory_safety_stock_with_lead_times\n",
    "          .filter(\n",
    "            f.expr('lead_time Is Not Null') # considering only non-null records\n",
    "             ).alias('b')\n",
    "        ), \n",
    "      on=f.expr('a.store_id=b.store_id AND a.sku=b.sku AND a.date > b.date'), \n",
    "      how='leftouter'\n",
    "      )\n",
    "    .groupBy('a.store_id','a.sku','a.date')\n",
    "      .agg(\n",
    "          f.max('a.lead_time').alias('lead_time'),\n",
    "          f.max('b.date').alias('lead_date'), # day on which the lead time was assigned to the inventory pipeline\n",
    "          f.max('b.lead_time').alias('prev_lead_time') # lead time assigned to the inventory pipeline\n",
    "        )\n",
    "  \n",
    "    # flag is raised if difference in current date and lead date (from above) is greater than the lead time assigned (prev_lead_time)\n",
    "    .withColumn('date_diff', f.expr('DATEDIFF(date, lead_date)'))\n",
    "    .withColumn('insufficient_lead_time', f.expr('''\n",
    "      CASE\n",
    "        WHEN lead_time IS NULL AND (prev_lead_time - date_diff) <= 0 THEN 1\n",
    "        ELSE 0\n",
    "        END\n",
    "      '''))\n",
    "    .select(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'insufficient_lead_time'\n",
    "      )\n",
    "  .join(\n",
    "    inventory_safety_stock_alert,\n",
    "    on=['store_id','sku','date']\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "lead_time_alerts.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d4623bf-a854-4dce-9477-94ea325b995f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Combining these conditions, we might flag out of stock situations that require attention as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3e634d-0810-4b84-b069-16e9a2afba9e",
     "showTitle": true,
     "title": "Consolidated Events"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 323:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+------------------+---------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units| daily_sales_units|alert_indicator|\n",
      "+----------+--------+---+----------------+-----------------+------------------+---------------+\n",
      "|2019-05-11|      63| 57|     Category 04|                0|0.8681318681318682|              0|\n",
      "|2019-09-12|      63| 57|     Category 04|                0|0.7032967032967034|              0|\n",
      "|2020-02-24|      63| 57|     Category 04|                3|0.9560439560439561|              0|\n",
      "|2020-10-25|      63| 57|     Category 04|                2|0.4945054945054945|              0|\n",
      "|2019-04-20|      98| 64|     Category 01|                8|1.4945054945054945|              1|\n",
      "+----------+--------+---+----------------+-----------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consolidated_oos_alerts = (\n",
    "  lead_time_alerts\n",
    "    .withColumn('alert_indicator', f.expr('''\n",
    "      CASE\n",
    "        WHEN on_hand_less_than_safety_stock = 1 AND insufficient_inventory_pipeline_units = 1 THEN 1 \n",
    "        WHEN on_hand_less_than_safety_stock = 1 AND insufficient_inventory_pipeline_units != 1 AND insufficient_lead_time = 1 THEN 1\n",
    "        ELSE 0\n",
    "        END'''))\n",
    "    .select(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'product_category',\n",
    "      'total_sales_units',\n",
    "      'daily_sales_units',\n",
    "      'alert_indicator'\n",
    "      )\n",
    "  )\n",
    "\n",
    "consolidated_oos_alerts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2770bb46-dc98-45bd-b970-bf685ca56746",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 7: Identify Zero Sales Issues\n",
    "\n",
    "Not every product sells each day in each store location. But when a product goes unsold for a long period of time, it might be wise for someone to verify it is still in inventory.  \n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/osa_tredence_zeroscan.png' width=75%>\n",
    "\n",
    "The challenge is that what constitutes a *long* period of time.  Some products move relatively quickly while many products move only occasionally.  For a product that sales hundreds of units daily, we might suspect an inventory problem if we suddenly see no sales on a given day.  For a product that moves only a few units a month, a few days or even a few weeks with no units sold may be nothing to be concerned with.\n",
    "\n",
    "With that in mind, we need to examine the number of days with zero units sold for each store-SKU combination relative to the total days a product is available for sale in order to understand the probability a product will experience a zero-sales day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0674bb-31ec-4480-bc67-59290f8d5836",
     "showTitle": true,
     "title": "Calculate Ratio of Zero Sales Days to Total Sales Days by Store-SKU"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+---------------------+--------------------------+\n",
      "|store_id|sku|total_days|total_zero_sales_days|zero_sales_day_probability|\n",
      "+--------+---+----------+---------------------+--------------------------+\n",
      "|    1540|155|       854|                  654|         0.765807962529274|\n",
      "|    1283| 39|       854|                  794|        0.9297423887587822|\n",
      "|    1822|111|       854|                  794|        0.9297423887587822|\n",
      "|     397| 64|       854|                  584|        0.6838407494145199|\n",
      "|     339| 64|       854|                  550|        0.6440281030444965|\n",
      "+--------+---+----------+---------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate ratio of total days of zero sales and total days on shelf across observed period\n",
    "zero_sales_totals = ( \n",
    "  inventory\n",
    "    .withColumn('total_zero_sales_days', f.expr('CASE WHEN total_sales_units == 0 THEN 1 ELSE 0 END'))\n",
    "    .withColumn('total_days', f.expr('1'))\n",
    "    .groupBy(['store_id', 'sku'])\n",
    "      .agg(\n",
    "        f.sum('total_days').alias('total_days'),\n",
    "        f.sum('total_zero_sales_days').alias('total_zero_sales_days')\n",
    "        )\n",
    "    .withColumn('zero_sales_day_probability', f.expr('total_zero_sales_days / total_days'))\n",
    "    )\n",
    "\n",
    "zero_sales_totals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d382e5-f22d-4355-83e9-bc87912038a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With these ratios in the back of our minds, we can now examine the number of consecutive days a product has experienced zero-units sold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c533c32-047e-4242-ac23-be30319746ed",
     "showTitle": true,
     "title": "Calculate Consecutive Zero Sales Days"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+-----------------+--------------------+---------------------+-------------------+\n",
      "|      date|store_id|sku|total_sales_units|zero_sales_flag_rank|sales_change_flag_inv|total_days_wo_sales|\n",
      "+----------+--------+---+-----------------+--------------------+---------------------+-------------------+\n",
      "|2019-01-01|      63| 57|                0|                   0|                    1|                  1|\n",
      "|2019-01-02|      63| 57|                0|                   0|                    1|                  2|\n",
      "|2019-01-03|      63| 57|                0|                   0|                    1|                  3|\n",
      "|2019-01-04|      63| 57|                0|                   0|                    1|                  4|\n",
      "|2019-01-05|      63| 57|                3|                   0|                    0|                  0|\n",
      "|2019-01-06|      63| 57|                2|                   0|                    0|                  0|\n",
      "|2019-01-07|      63| 57|                0|                   1|                    1|                  1|\n",
      "|2019-01-08|      63| 57|                0|                   1|                    1|                  2|\n",
      "|2019-01-09|      63| 57|                3|                   1|                    0|                  0|\n",
      "|2019-01-10|      63| 57|                2|                   1|                    0|                  0|\n",
      "+----------+--------+---+-----------------+--------------------+---------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_sales_days = (\n",
    "  \n",
    "  inventory \n",
    "  \n",
    "    # flag the occurance of first zero sales day in a series\n",
    "    .withColumn('sales_change_flag', f.expr('''\n",
    "        CASE \n",
    "          WHEN total_sales_units=0 AND LAG(total_sales_units,1) OVER(PARTITION BY store_id, sku ORDER BY date) != 0 THEN 1 \n",
    "          ELSE 0 \n",
    "          END''')) \n",
    "  \n",
    "    # count number of zero sales day series to date (associates records with a given series)\n",
    "    .withColumn('zero_sales_flag_rank', f.expr('SUM(sales_change_flag) OVER(PARTITION BY store_id, sku ORDER BY date)')) \n",
    "  \n",
    "    # flag all zero sales days\n",
    "    .withColumn('sales_change_flag_inv', f.expr('CASE WHEN total_sales_units = 0 THEN 1 ELSE 0 END')) \n",
    "  \n",
    "    # count consecutive zero sales days (counter resets with a non-zero sales instance)\n",
    "    .withColumn('total_days_wo_sales', f.expr('SUM(sales_change_flag_inv) OVER(PARTITION BY store_id, sku, zero_sales_flag_rank ORDER BY date)'))\n",
    "    .withColumn('total_days_wo_sales', f.expr('CASE WHEN total_sales_units != 0 THEN 0 ELSE total_days_wo_sales END'))\n",
    "    \n",
    "    .select(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'total_sales_units',\n",
    "      'zero_sales_flag_rank',\n",
    "      'sales_change_flag_inv',\n",
    "      'total_days_wo_sales'\n",
    "      )\n",
    "  )\n",
    "\n",
    "zero_sales_days.orderBy('store_id','sku','date').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be2a933-6ebd-4a38-9866-2223c3e73611",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We know the probability that a product will experience a zero-sales date in a given store location.  Using this value, we can calculate the probability of a product would realistically experience consecutive zero-sales days using a simple function.  When that probability reaches or exceeds a particular threshold (set here at 5% to reflect a fairly low risk tolerance), we might take that as a signal to send someone to inspect the inventory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10063464-a56e-43c9-8a26-6a06b8624a32",
     "showTitle": true,
     "title": "Calculate Cumulative Probability of Zero Sales Event"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------------+--------------------+---------------------+-------------------+----------+---------------------+--------------------------+----------------------+-------------+\n",
      "|store_id|sku|      date|total_sales_units|zero_sales_flag_rank|sales_change_flag_inv|total_days_wo_sales|total_days|total_zero_sales_days|zero_sales_day_probability|zero_sales_probability|no_sales_flag|\n",
      "+--------+---+----------+-----------------+--------------------+---------------------+-------------------+----------+---------------------+--------------------------+----------------------+-------------+\n",
      "|      63| 57|2019-01-01|                0|                   0|                    1|                  1|       854|                  654|         0.765807962529274|     0.765807962529274|            0|\n",
      "|      63| 57|2019-01-02|                0|                   0|                    1|                  2|       854|                  654|         0.765807962529274|    0.5864618354732378|            0|\n",
      "|      63| 57|2019-01-03|                0|                   0|                    1|                  3|       854|                  654|         0.765807962529274|    0.4491171433249386|            0|\n",
      "|      63| 57|2019-01-04|                0|                   0|                    1|                  4|       854|                  654|         0.765807962529274|   0.34393748446663913|            0|\n",
      "|      63| 57|2019-01-05|                3|                   0|                    0|                  0|       854|                  654|         0.765807962529274|                   1.0|            0|\n",
      "+--------+---+----------+-----------------+--------------------+---------------------+-------------------+----------+---------------------+--------------------------+----------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_sales_inventory = (\n",
    "  zero_sales_days\n",
    "    .join(zero_sales_totals.alias('prob'), on=['store_id', 'sku'], how = 'leftouter')\n",
    "    .withColumn('zero_sales_probability', f.expr('pow(zero_sales_day_probability, total_days_wo_sales)'))\n",
    "    .withColumn('no_sales_flag', f.expr('CASE WHEN zero_sales_probability < 0.05 THEN 1 ELSE 0 END'))\n",
    "  )\n",
    "\n",
    "zero_sales_inventory.orderBy('store_id','sku','date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c19c01-a993-467e-8e8f-8dc62fddc8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 8: Identify Alert Conditions\n",
    "\n",
    "We now have identified several conditions that require attention.  We first identified problematic phantom inventory conditions and then identified inventory below safety stock levels.  Finally, we identified days with zero sales events likely to be a result of an inventory issue.  In this last step, we'll consolidate all this information to build a set with which we can more clearly identify inventory dates requiring attention from analysts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba83d196-5175-442c-95a3-08b01d90c4b8",
     "showTitle": true,
     "title": "Combining all flags"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 420:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+----------------+-----------------+---------+------------------+---------------+-----------------+---------------------+\n",
      "|      date|store_id|sku|product_category|total_sales_units|oos_alert| daily_sales_units|zero_sales_flag|phantom_inventory|phantom_inventory_ind|\n",
      "+----------+--------+---+----------------+-----------------+---------+------------------+---------------+-----------------+---------------------+\n",
      "|2019-05-11|      63| 57|     Category 04|                0|        0|0.8681318681318682|              0|                0|                    0|\n",
      "|2019-09-12|      63| 57|     Category 04|                0|        0|0.7032967032967034|              0|                0|                    0|\n",
      "|2020-02-24|      63| 57|     Category 04|                3|        0|0.9560439560439561|              0|               -1|                    0|\n",
      "|2020-10-25|      63| 57|     Category 04|                2|        0|0.4945054945054945|              0|              -37|                    1|\n",
      "|2019-04-20|      98| 64|     Category 01|                8|        1|1.4945054945054945|              0|              -18|                    1|\n",
      "+----------+--------+---+----------------+-----------------+---------+------------------+---------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_alerts = (\n",
    "  consolidated_oos_alerts.alias('oos') # OOS alert\n",
    "    .join(phantom_inventory.alias('pi'), on=['store_id','sku','date'], how='leftouter') # pahntom inventory indicator\n",
    "    .join(zero_sales_inventory, on=['store_id','sku','date'], how='leftouter') # zero sales alert\n",
    "    .selectExpr(\n",
    "      'date',\n",
    "      'store_id',\n",
    "      'sku',\n",
    "      'product_category',\n",
    "      'oos.total_sales_units',\n",
    "      'oos.alert_indicator as oos_alert',\n",
    "      'oos.daily_sales_units',\n",
    "      'no_sales_flag as zero_sales_flag',\n",
    "      'phantom_inventory',\n",
    "      'phantom_inventory_ind'\n",
    "      )\n",
    "  )\n",
    "\n",
    "all_alerts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f531ecd2-f782-4d3f-b2f7-b327fbcb31b7",
     "showTitle": true,
     "title": "Persist Flagged Inventory Data for Future Use"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (\n",
    "#   all_alerts\n",
    "#     .repartition(3)\n",
    "#     .write\n",
    "#       .format('parquet')\n",
    "#       .mode('overwrite')\n",
    "#       .option('overwriteSchema', 'true')\n",
    "#       .saveAsTable('osa_inventory_flagged')\n",
    "#    )\n",
    "all_alerts.createOrReplaceTempView(\"osa_inventory_flagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c368e2-3525-4fa8-a444-55a06d8851b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 9: Access Data\n",
    "\n",
    "In the last notebook, we identified inventory problems through the detection of excessive phantom inventory, stocking levels below safety stock thresholds and unexpected numbers of consecutive days with zero sales.  We might label these *out-of-stock* issues in that they identify scenarios where product is simply not available to be sold.\n",
    "\n",
    "In this notebook, we want to add a fourth inventory scenario, one where insufficient stock may not fully prevent sales but where they may cause us to miss our sales expectations.  Misplacement of product in the store or displays which give the customer a sense a product is not available are both examples of the kinds of issues we might describe as *on-shelf availability* problems.\n",
    "\n",
    "With this fourth scenario, we will generate a forecast for sales and identify historical sales values that were depressed relative to what was expected. These periods of lower than expected sales may then be investigated as periods potentially experiencing OSA challenges. To generate this forecast, we must first access our historical sales data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e17a097e-ea93-4a32-b39d-a36363862e94",
     "showTitle": true,
     "title": "Import Required Libraries"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\t\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6871d6e8-6ae6-4526-9cb3-146079888bb6",
     "showTitle": true,
     "title": "Read the Input Datasets"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inventory_flagged = spark.table('osa_inventory_flagged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d471149-8bba-4ba4-be24-7a4765b6b59b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 10: Generate Forecast\n",
    "\n",
    "Unlike most forecasting exercises, our goal is not to predict future values but instead to generate *expected* values for the historical period.  To do this, we may make use of a variety of forecasting techniques. Most enterprises already have established preferences for sales forecasting so instead of wading into the conversation about which techniques are best in different scenarios, we will make use of a [simple exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing) as a placeholder technique so that we might focus on the analysis against the forecasted values in later steps.\n",
    "\n",
    "Our challenge now is to generate a forecast for each store-SKU combination in our dataset.  Leveraging a forecast scaling technique [previously demonstrated](https://databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html), we will write a function capable of generating a forecast for a given store-SKU combination and then apply it to all store-SKU combinations in our dataset in a scalable, distributed manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e95f2bfa-976f-4df0-a2e0-1ab9a554c5b7",
     "showTitle": true,
     "title": "Define Function to Generate Forecast for a Store-SKU"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha_value = 0.8 # smoothing factor\n",
    "\n",
    "# function to generate a forecast for a store-sku\n",
    "def get_forecast(keys, inventory_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "  \n",
    "  # identify store and sku\n",
    "  store_id = keys[0]\n",
    "  sku = keys[1]\n",
    "  \n",
    "  # identify date range for predictions\n",
    "  history_start = inventory_pd['date'].min()\n",
    "  history_end = inventory_pd['date'].max()\n",
    "  \n",
    "  # organize data for model training\n",
    "  with warnings.catch_warnings(record=True):\n",
    "      timeseries = (\n",
    "        inventory_pd\n",
    "          .set_index('date', drop=True, append=False) # move date to index\n",
    "          .sort_index() # sort on date-index\n",
    "        )['total_sales_units'] # just need this one field\n",
    "\n",
    "  # fit model to timeseries\n",
    "  model = SimpleExpSmoothing(timeseries, initialization_method='heuristic').fit(smoothing_level=alpha_value, optimized=False)\n",
    "  \n",
    "  # predict sales across historical period\n",
    "  predictions = model.predict(start=history_start, end=history_end)\n",
    "  \n",
    "  # convert timeseries to dataframe for return\n",
    "  predictions_pd = predictions.to_frame(name='predicted_sales_units').reset_index() # convert to df\n",
    "  predictions_pd.rename(columns={'index':'date'}, inplace=True) # rename 'index' column to 'date'\n",
    "  predictions_pd['store_id'] = store_id # assign store id\n",
    "  predictions_pd['sku'] = sku # assign sku\n",
    "  \n",
    "  return predictions_pd[['date', 'store_id', 'sku', 'predicted_sales_units']]\n",
    "\n",
    "# structure of forecast function output\n",
    "forecast_schema = StructType([\n",
    "  StructField('date', DateType()), \n",
    "  StructField('store_id', IntegerType()), \n",
    "  StructField('sku', IntegerType()), \n",
    "  StructField('predicted_sales_units', FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f825ea1-df4d-4713-be36-7875d9b66576",
     "showTitle": true,
     "title": "Generate Forecasts for All Store-SKUs"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 480:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---------------------+\n",
      "|      date|store_id|sku|predicted_sales_units|\n",
      "+----------+--------+---+---------------------+\n",
      "|2019-01-01|      63| 57|                  0.0|\n",
      "|2019-01-02|      63| 57|                  0.0|\n",
      "|2019-01-03|      63| 57|                  0.0|\n",
      "|2019-01-04|      63| 57|                  0.0|\n",
      "|2019-01-05|      63| 57|                  0.0|\n",
      "|2019-01-06|      63| 57|                  2.0|\n",
      "|2019-01-07|      63| 57|                  2.0|\n",
      "|2019-01-08|      63| 57|                  0.0|\n",
      "|2019-01-09|      63| 57|                  0.0|\n",
      "|2019-01-10|      63| 57|                  2.0|\n",
      "+----------+--------+---+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# get forecasted values for each store-sku combination\n",
    "\n",
    "forecast = (\n",
    "  inventory_flagged\n",
    "    .groupby(['store_id','sku'])\n",
    "      .applyInPandas(\n",
    "        get_forecast, \n",
    "        schema=forecast_schema\n",
    "        )\n",
    "    .withColumn('predicted_sales_units', f.expr('ROUND(predicted_sales_units,0)')) # round values to nearest integer\n",
    "    )\n",
    "\n",
    "forecast.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21e1f02-e017-4131-8f7f-bbf5d0e7b10e",
     "showTitle": true,
     "title": "Persist Forecasts"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "forecast.createOrReplaceTempView(\"osa_inventory_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0ff694-0f0c-4ef5-b6fd-04d0925e6454",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 11: Identify *Off* Sales Issues\n",
    "\n",
    "With forecasts in-hand, we will now look for historical periods where there is not only a lower than expected number of sales (relative to our forecasts) but where this difference grows over a number of days. Identifying these periods may help us identify on-shelf availability (OSA) concerns we may need to address.  \n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/osa_tredence_offsales.jpg' width=75%>\n",
    "\n",
    "Of course, not every missed sales target is an OSA event.  To focus our attention, we will look for periods of sustained misses where the miss is sizeable relative to our expectations.  In the code that follows, we require 4-days of increasing misses with an average daily miss of 20% or more of the expected sales. Some organizations may wish to increase or decrease these threshold requirements depending on the nature of their business:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4313e553-5612-4169-8a2f-c169bd9b1e77",
     "showTitle": true,
     "title": "Flag Off-Sales Events"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/root/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|      date|store_id|sku|predicted_sales_units|off_sales_alert|oos_alert|zero_sales_flag|phantom_inventory|phantom_inventory_ind|\n",
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|2019-01-01|      63| 57|                  0.0|              0|        0|              0|             NULL|                    0|\n",
      "|2019-01-02|      63| 57|                  0.0|              0|        0|              0|                0|                    0|\n",
      "|2019-01-03|      63| 57|                  0.0|              0|        0|              0|                0|                    0|\n",
      "|2019-01-04|      63| 57|                  0.0|              0|        0|              0|                1|                    1|\n",
      "|2019-01-05|      63| 57|                  0.0|              0|        0|              0|                7|                    1|\n",
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_forecast = spark.table('osa_inventory_forecast')\n",
    "\n",
    "osa_flag_output = (\n",
    "  \n",
    "  inventory_flagged.alias('inv')\n",
    "    .join(inventory_forecast.alias('for'), on=['store_id','sku','date'], how='leftouter')\n",
    "    .selectExpr(\n",
    "      'inv.*',\n",
    "      'for.predicted_sales_units'\n",
    "      )\n",
    "             \n",
    "    # calculating difference between forecasted and actual sales units\n",
    "    .withColumn('units_difference', f.expr('predicted_sales_units - total_sales_units'))\n",
    "    .withColumn('units_difference', f.expr('COALESCE(units_difference, 0)'))\n",
    "\n",
    "    # check whether deviation has been increasing over past 4 days\n",
    "    .withColumn('osa_alert_inc_deviation', f.expr('''\n",
    "      CASE \n",
    "        WHEN units_difference > LAG(units_difference, 1) OVER(PARTITION BY store_id, sku ORDER BY date) AND \n",
    "             LAG(units_difference, 1) OVER(PARTITION BY store_id, sku ORDER BY date) > LAG(units_difference, 2) OVER(PARTITION BY store_id, sku ORDER BY date) AND \n",
    "             LAG(units_difference, 2) OVER(PARTITION BY store_id, sku ORDER BY date) > LAG(units_difference, 3) OVER(PARTITION BY store_id, sku ORDER BY date)\n",
    "             THEN 1\n",
    "        ELSE 0 \n",
    "        END'''))\n",
    "    .withColumn('osa_alert_inc_deviation', f.expr('COALESCE(osa_alert_inc_deviation, 0)'))\n",
    "\n",
    "    # rolling 4 day average of sales units\n",
    "    .withColumn('sales_4day_avg', f.expr('AVG(total_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)'))\n",
    "\n",
    "    # rolling 4 day average of forecasted units\n",
    "    .withColumn('predictions_4day_avg', f.expr('AVG(predicted_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)'))\n",
    "\n",
    "    # calculating deviation in rolling average of sales and forecast units\n",
    "    .withColumn('deviation', f.expr('(predictions_4day_avg - sales_4day_avg) / (predictions_4day_avg+1)'))\n",
    "    .withColumn('deviation', f.expr('COALESCE(deviation, 0)'))\n",
    "\n",
    "    # Considering 20% deviation as the threshold for OSA flag\n",
    "    .withColumn('off_sales_alert', f.expr('''\n",
    "      CASE \n",
    "        WHEN deviation > 0.20  AND osa_alert_inc_deviation = 1 THEN 1\n",
    "        ELSE 0\n",
    "        END'''))\n",
    "\n",
    "    .select('date', \n",
    "            'store_id', \n",
    "            'sku', \n",
    "            'predicted_sales_units', \n",
    "            'off_sales_alert',\n",
    "            'oos_alert', \n",
    "            'zero_sales_flag', \n",
    "            'phantom_inventory', \n",
    "            'phantom_inventory_ind')\n",
    "    )\n",
    "\n",
    "osa_flag_output.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END\n",
    "Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 145370659648331,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Data Preparation",
   "notebookOrigID": 1211232247746774,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
