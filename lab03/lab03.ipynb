{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34cecd7",
   "metadata": {},
   "source": [
    "# Lab 3: Spark Structured APIs\n",
    "\n",
    "In this class, we will learn about the Spark Structured APIs, including DataFrame APIs and Basic Spark SQL operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18b7f2",
   "metadata": {},
   "source": [
    "## 1. About Jupyter Notebook\n",
    "\n",
    "Enter the IP and your jupyter port in the web browser. For example: `172.18.30.207:11223` and enter the default jupyter password. \n",
    "\n",
    "1. Check the default jupyter config by `jupyter lab --generate-config`.\n",
    "\n",
    "2. The default jupyter file path is `/data/lab`.\n",
    "\n",
    "3. If you want to select the right jupyter kernel. \n",
    "\n",
    "* Alternative method to install jupyter notebook in your conda env. [[link](https://zhuanlan.zhihu.com/p/107567637)]\n",
    "* More about different jupyter kernels. [[link](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62acafb-58bb-4b49-ba0f-ee1f35cc360e",
   "metadata": {},
   "source": [
    "## 2. About Spark installation\n",
    "\n",
    "* Spark path `/opt/module/spark-3.5.0-bin-hadoop3/`\n",
    "* List pyspark arguements by `pyspark --help`\n",
    "* [Install spark from source](https://spark.apache.org/docs/3.5.1/building-spark.html)\n",
    "\n",
    "Use pyspark from command line\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=ipython\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"--matplotlib\"\n",
    "/opt/module/spark-3.5.0-bin-hadoop3/bin/pyspark -h\n",
    "\n",
    "```\n",
    "\n",
    "Use pyspark from jupyter notebook, in your command line, change settings back to original settings.\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='lab'\n",
    "\n",
    "```\n",
    "\n",
    "## ^_^ Here is the [Spark documentation](https://spark.apache.org/docs/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e867d1b-6134-4faa-bcee-a74f958bf848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ce191-73bf-45c7-8088-3c4fba3bad5d",
   "metadata": {},
   "source": [
    "## 3. Install pyspark with different version\n",
    "\n",
    "* Create conda env `sp` with `conda create -n sp python=3.11.7`\n",
    "* init conda env with `conda init`\n",
    "* activate conda env with `conda activate sp` or `source activate sp`\n",
    "* Install pyspark with different version `pip install pyspark==3.5.0`  , -i change the mirror\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc623d7e-ca54-4055-9a65-854504df9c6e",
   "metadata": {},
   "source": [
    "## 4. Try databricks\n",
    "\n",
    "* [databricks sign up](https://www.databricks.com/try-databricks#account), select Databricks Community Edition. \n",
    "* [databricks resources](https://www.databricks.com/resources)\n",
    "* [databricks cases](https://github.com/orgs/databricks-industry-solutions/repositories)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e131a",
   "metadata": {},
   "source": [
    "## 5. Basic DataFrame Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d519b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9d6042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 10:15:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## change the spark.ui.port to your own 4040 port. \n",
    "spark = SparkSession.builder.config('spark.ui.port', 64050).appName(\"pyspark SQL basic example\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"/shareddata/data/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5553e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7b7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fa76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ec5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     NULL|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca6a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c34841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|NULL|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26a798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people_tmp\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people_tmp\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae64815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createOrReplaceGlobalTempView(\"people_global\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people_global\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ad306",
   "metadata": {},
   "source": [
    "这行代码在DataFrame `df`中创建了一个新的列，名为\"This Long Column-Name\"，其内容是原有'name'列的内容。\n",
    "\n",
    "这行代码选择了名为\"This Long Column-Name\"的列，并显示了其内容。注意，由于列名中包含空格，所以在选择列时需要使用反引号（`）将列名括起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043e0037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------------------+\n",
      "| age|   name|This Long Column-Name|\n",
      "+----+-------+---------------------+\n",
      "|NULL|Michael|              Michael|\n",
      "|  30|   Andy|                 Andy|\n",
      "|  19| Justin|               Justin|\n",
      "+----+-------+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|This Long Column-Name|\n",
      "+---------------------+\n",
      "|              Michael|\n",
      "|                 Andy|\n",
      "|               Justin|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"This Long Column-Name\", F.col('name'))\n",
    "df.show()\n",
    "df.selectExpr(\"`This Long Column-Name`\").show()  ## Note the ` symbol. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e12734",
   "metadata": {},
   "source": [
    "`select`和`selectExpr`都是PySpark DataFrame的方法，用于选择DataFrame中的列，但它们的使用方式和功能有些不同。\n",
    "\n",
    "`select`方法接受一系列列名作为参数，返回一个新的DataFrame，只包含指定的列。例如：\n",
    "\n",
    "```python\n",
    "df.select(\"name\", \"age\").show()\n",
    "```\n",
    "\n",
    "这将返回一个新的DataFrame，只包含\"name\"和\"age\"两列。\n",
    "\n",
    "而`selectExpr`方法则更加强大，它接受一系列表达式作为参数，这些表达式可以包含SQL风格的操作，如算术运算、聚合函数等。例如：\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"name\", \"age * 2\").show()\n",
    "```\n",
    "\n",
    "这将返回一个新的DataFrame，包含\"name\"列和\"age\"列的两倍。\n",
    "\n",
    "在你的例子中，`selectExpr(\"`This Long Column-Name`\")`使用了反引号来引用包含空格的列名，这是SQL语法的一部分，`select`方法则不能这样做。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7e837",
   "metadata": {},
   "source": [
    "## 6. Case Studies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f38118",
   "metadata": {},
   "source": [
    "\n",
    "#### (1). Line count\n",
    "\n",
    "Count the lines in `data/SPARK_README.md`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef7f663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|line                                                                          |\n",
      "+------------------------------------------------------------------------------+\n",
      "|# Apache Spark                                                                |\n",
      "|                                                                              |\n",
      "|Spark is a fast and general cluster computing system for Big Data. It provides|\n",
      "|high-level APIs in Scala, Java, Python, and R, and an optimized engine that   |\n",
      "|supports general computation graphs for data analysis. It also supports a     |\n",
      "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,    |\n",
      "|MLlib for machine learning, GraphX for graph processing,                      |\n",
      "|and Spark Streaming for stream processing.                                    |\n",
      "|                                                                              |\n",
      "|<http://spark.apache.org/>                                                    |\n",
      "+------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "the file has 95 lines\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, instr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark case study\").getOrCreate()\n",
    "\n",
    "df = spark.read.text(\"/shareddata/data/SPARK_README.md\").toDF(\"line\")\n",
    "df.show(10, truncate=False)\n",
    "print(f\"the file has {df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f770241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered text has 17 lines\n",
      "+--------------------+------+\n",
      "|                line|result|\n",
      "+--------------------+------+\n",
      "|      # Apache Spark|  true|\n",
      "|Spark is a fast a...|  true|\n",
      "|rich set of highe...|  true|\n",
      "|and Spark Streami...|  true|\n",
      "|You can find the ...|  true|\n",
      "|   ## Building Spark|  true|\n",
      "|Spark is built us...|  true|\n",
      "|To build Spark an...|  true|\n",
      "|[\"Building Spark\"...|  true|\n",
      "|The easiest way t...|  true|\n",
      "|Spark also comes ...|  true|\n",
      "|    ./bin/run-exa...|  true|\n",
      "|    MASTER=spark:...|  true|\n",
      "|Testing first req...|  true|\n",
      "|Spark uses the Ha...|  true|\n",
      "|Hadoop, you must ...|  true|\n",
      "|in the online doc...|  true|\n",
      "+--------------------+------+\n",
      "\n",
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a fast a...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered = df.withColumn(\"result\", instr(col('line'), 'Spark')>=1).where('result')\n",
    "print(f\"filtered text has {filtered.count()} lines\")\n",
    "filtered.show(20)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eee7b351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                line|results_and|\n",
      "+--------------------+-----------+\n",
      "|Spark uses the Ha...|       true|\n",
      "+--------------------+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a fast a...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containsubstr1 = instr(col(\"line\"), \"Spark\") >= 1\n",
    "containsubstr2 = instr(col(\"line\"), \"talk\") >= 1\n",
    "filtered2 = df.withColumn(\"results_and\", containsubstr1 & containsubstr2).where('results_and')\n",
    "filtered2.show()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c1a45",
   "metadata": {},
   "source": [
    "Question: if we want to filter lines that contain *any word* from a long list, what should we do?\n",
    "\n",
    "For example, `candidates = ['mesos', 'guidance', 'particular', 'Hadoop', 'setup', 'project']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adddffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5625d1c",
   "metadata": {},
   "source": [
    "#### (2). mnm count\n",
    "\n",
    "The data is in `data/mnm_dataset.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fa35b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 12:05:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Count|\n",
      "+-----+------+-----+\n",
      "|   TX|   Red|   20|\n",
      "|   NV|  Blue|   66|\n",
      "|   CO|  Blue|   79|\n",
      "|   OR|  Blue|   71|\n",
      "|   WA|Yellow|   93|\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mnm_count\").getOrCreate()\n",
    "\n",
    "mnm_file = \"/shareddata/data/mnm_dataset.csv\"\n",
    "mnm_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(mnm_file)\n",
    "\n",
    "mnm_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48cea93",
   "metadata": {},
   "source": [
    "Aggregate count of all colors and groupBy state and color, orderBy descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9369aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "+-----+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total Rows = 60\n"
     ]
    }
   ],
   "source": [
    "count_mnm_df = mnm_df.select(\"State\", \"Color\", \"Count\").groupBy(\"State\", \"Color\").agg(count(\"Count\").alias(\"Total\")).orderBy(\"Total\", ascending=False)\n",
    "count_mnm_df.show(n=10, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f4940",
   "metadata": {},
   "source": [
    "Find the aggregate count for California by filtering on State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "420bfb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|State|Color|Total|\n",
      "+-----+-----+-----+\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## code here\n",
    "count_mnm_df.where('\"State\" == \"CA\" ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898d8df",
   "metadata": {},
   "source": [
    "#### (3) San Francisco Fire Calls\n",
    "\n",
    "Showing how to use DataFrame and Spark SQL for common data analytics patterns and operations on a [San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b59f4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 12:40:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"fire_calls\").getOrCreate()\n",
    "sf_fire_file = \"/shareddata/data/sf-fire/sf-fire-calls.csv\"\n",
    "\n",
    "# Define our schema as the file has 4 million records. Inferring the schema is expensive for large files.\n",
    "\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                     StructField('UnitID', StringType(), True),\n",
    "                     StructField('IncidentNumber', IntegerType(), True),\n",
    "                     StructField('CallType', StringType(), True),                  \n",
    "                     StructField('CallDate', StringType(), True),      \n",
    "                     StructField('WatchDate', StringType(), True),\n",
    "                     StructField('CallFinalDisposition', StringType(), True),\n",
    "                     StructField('AvailableDtTm', StringType(), True),\n",
    "                     StructField('Address', StringType(), True),       \n",
    "                     StructField('City', StringType(), True),       \n",
    "                     StructField('Zipcode', IntegerType(), True),       \n",
    "                     StructField('Battalion', StringType(), True),                 \n",
    "                     StructField('StationArea', StringType(), True),       \n",
    "                     StructField('Box', StringType(), True),       \n",
    "                     StructField('OriginalPriority', StringType(), True),       \n",
    "                     StructField('Priority', StringType(), True),       \n",
    "                     StructField('FinalPriority', IntegerType(), True),       \n",
    "                     StructField('ALSUnit', BooleanType(), True),       \n",
    "                     StructField('CallTypeGroup', StringType(), True),\n",
    "                     StructField('NumAlarms', IntegerType(), True),\n",
    "                     StructField('UnitType', StringType(), True),\n",
    "                     StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "                     StructField('FirePreventionDistrict', StringType(), True),\n",
    "                     StructField('SupervisorDistrict', StringType(), True),\n",
    "                     StructField('Neighborhood', StringType(), True),\n",
    "                     StructField('Location', StringType(), True),\n",
    "                     StructField('RowID', StringType(), True),\n",
    "                     StructField('Delay', FloatType(), True)])\n",
    "\n",
    "\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b45b3d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 12:41:11 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[CallNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallDate: string, WatchDate: string, CallFinalDisposition: string, AvailableDtTm: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: string, Box: string, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceInCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: string, Neighborhood: string, Location: string, RowID: string, Delay: float]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cache the DataFrame since we will be performing some operations on it.\n",
    "fire_df.cache()\n",
    "# fire_df.count()\n",
    "# fire_df.printSchema()\n",
    "# fire_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbd9bf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out \"Medical Incident\" call types\n",
    "\n",
    "few_fire_df = (fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "              .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29da8e",
   "metadata": {},
   "source": [
    "**Q-1) How many distinct types of calls were made to the Fire Department? (exclude the null strings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b29c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917972b",
   "metadata": {},
   "source": [
    "**Q-2) What are distinct types of calls were made to the Fire Department?**\n",
    "\n",
    "These are all the distinct type of call to the SF Fire Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5df5f3",
   "metadata": {},
   "source": [
    "**Q-3) Find out all response or delayed times greater than 5 mins?**\n",
    "\n",
    "1. Rename the column `Delay` - > `ReponseDelayedinMins`\n",
    "2. Returns a new DataFrame\n",
    "3. Find out all calls where the response time to the fire site was delayed for more than 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e017265",
   "metadata": {},
   "source": [
    "Transform the string dates to Spark Timestamp data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e969642",
   "metadata": {},
   "source": [
    "**Q-4) What were the most common call types?**\n",
    "\n",
    "List them in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003700a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb173f",
   "metadata": {},
   "source": [
    "**Q-4a) What zip codes accounted for most common calls?**\n",
    "\n",
    "Let's investigate what zip codes in San Francisco accounted for most fire calls and what type where they.\n",
    "\n",
    "1. Filter out by CallType\n",
    "2. Group them by CallType and Zip code\n",
    "3. Count them and display them in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e4144",
   "metadata": {},
   "source": [
    "**Q-4b) What San Francisco neighborhoods are in the zip codes 94102 and 94103**\n",
    "\n",
    "Let's find out the neighborhoods associated with these two zip codes. In all likelihood, these are some of the contested neighborhood with high reported crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaa685",
   "metadata": {},
   "source": [
    "**Q-5) What was the sum of all calls, average, min and max of the response times for calls?**\n",
    "\n",
    "* Number of Total Alarms\n",
    "* What were the min and max the delay in response time before the Fire Dept arrived at the scene of the call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce471cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb085940",
   "metadata": {},
   "source": [
    "**Q-6a) How many distinct years of data is in the CSV file?**\n",
    "\n",
    "We can use the `year()` SQL Spark function off the Timestamp column data type IncidentDate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777b018",
   "metadata": {},
   "source": [
    "**Q-6b) What week of the year in 2018 had the most fire calls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1051a3c",
   "metadata": {},
   "source": [
    "**Q-7) What neighborhoods in San Francisco had the worst response time in 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287bc70",
   "metadata": {},
   "source": [
    "**Q-8a) How can we use Parquet files or SQL table to store data and read it back?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e5031",
   "metadata": {},
   "source": [
    "**Q-8c) How can read data from Parquet file?**\n",
    "\n",
    "Note we don't have to specify the schema here since it's stored as part of the Parquet metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c20d61",
   "metadata": {},
   "source": [
    "#### (4) US Flights Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100aadb",
   "metadata": {},
   "source": [
    "Define a UDF to convert the date format into a legible format.\n",
    "\n",
    "*Note*: the date is a string with year missing, so it might be difficult to do any queries using SQL `year()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0da42b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_date_format_udf(d_str):\n",
    "  l = [char for char in d_str]\n",
    "  return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])\n",
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e500609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF\n",
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .schema(\"date STRING, delay INT, distance INT, origin STRING, destination STRING\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"path\", \"/shareddata/data/flights/departuredelays.csv\")\n",
    "      .load())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b36e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"to_date_format_udf(date) as data_format\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1c0e5",
   "metadata": {},
   "source": [
    "\n",
    "Create a temporary view to which we can issue SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f11d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347cee1",
   "metadata": {},
   "source": [
    "Convert all `date` to `date_fm` so it's more eligible\n",
    "Note: we are using UDF to convert it on the fly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c73fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT *, date, to_date_format_udf(date) AS date_fm FROM us_delay_flights_tbl\").show(10, truncate=False)\n",
    "spark.sql(\"SELECT COUNT(*) FROM us_delay_flights_tbl\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19d146",
   "metadata": {},
   "source": [
    "Query 1:  Find out all flights whose distance between origin and destination is greater than 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d17601",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT distance, origin, destination FROM us_delay_flights_tbl WHERE distance > 1000 ORDER BY distance DESC\").show(10, truncate=False)\n",
    "\n",
    "## or \n",
    "df.select(\"distance\", \"origin\", \"destination\").where(col(\"distance\") > 1000).orderBy(desc(\"distance\")).show(10, truncate=False)\n",
    "\n",
    "\n",
    "df.select(\"distance\", \"origin\", \"destination\").where(\"distance > 1000\").orderBy(\"distance\", ascending=False).show(10)\n",
    "\n",
    "\n",
    "df.select(\"distance\", \"origin\", \"destination\").where(\"distance > 1000\").orderBy(desc(\"distance\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5f23c",
   "metadata": {},
   "source": [
    "Query 2: Find out all flights with 2 hour delays between San Francisco and Chicago  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f54664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT date, delay, origin, destination \n",
    "    FROM us_delay_flights_tbl \n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "    ORDER by delay DESC\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab29023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =  spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df1.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_tmp_view\")\n",
    "spark.catalog.listTables(dbName=\"global_temp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d8999",
   "metadata": {},
   "source": [
    "#### (5) Max purchase quantity over all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/shareddata/data/retail-data/*.csv\")\\\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/dd/yyyy HH:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window\\\n",
    "  .partitionBy(\"CustomerId\", \"date\")\\\n",
    "  .orderBy(desc(\"Quantity\"))\\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64983cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de537d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a96b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769487c0",
   "metadata": {},
   "source": [
    "# END\n",
    "\n",
    "# Thank you "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
