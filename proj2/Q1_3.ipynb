{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import T5Transformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os \n",
    "os.environ['PYSPARK_PYTHON'] = \"/root/anaconda3/bin/python\"     # 设置了运行 PySpark 的 Python 解释器的环境变量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json \n",
    "\n",
    "broker = 'localhost:64050'\n",
    "consumer = KafkaConsumer('q1_3', \n",
    "        bootstrap_servers=broker, \n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')), \n",
    "        consumer_timeout_ms=100000,\n",
    "        # auto_offset_reset='earliest', enable_auto_commit=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark2 = SparkSession.builder\\\n",
    "#     .config(\\\n",
    "#         \"spark.jars\",  \\\n",
    "#         \"/shareddata/lab09/kafka-clients-3.5.0.jar, \\\n",
    "#         /shareddata/lab09/spark-sql-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "#         /shareddata/lab09/spark-token-provider-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "#         /shareddata/lab09/commons-pool2-2.12.0.jar\") \\\n",
    "#     .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/anaconda3/bin/python\")\\\n",
    "#     .config(\"spark.executor.memory\", \"16g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"16g\") \\\n",
    "#     .config(\"spark.log.level\", \"WARN\") \\\n",
    "#     .appName(\"q1_3\").getOrCreate()\n",
    "\n",
    "# df = spark2.readStream.format(\"kafka\")\\\n",
    "# \t.option(\"kafka.bootstrap.servers\", \"localhost:64050\")\\\n",
    "# \t.option(\"subscribe\", \"q1_3\")\\\n",
    "# \t.option(\"startingOffsets\", \"earliest\")\\\n",
    "#     .load()\n",
    "# df.printSchema()\n",
    "\n",
    "# df1 = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\",\"CAST (timestamp AS TIMESTAMP)\")\n",
    "# schema = \"input String, output String\"\n",
    "# result = df1.selectExpr(\"key\", \"from_json(value, '{}') AS data\".format(schema), \"timestamp\")\n",
    "# q3_query = result.writeStream.queryName(\"QA\").format(\"memory\").outputMode(\"update\").trigger(processingTime=\"5 seconds\").start()\n",
    "\n",
    "# data_df = spark2.sql(\"SELECT data as text from QA\")\n",
    "# data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/module/spark-3.5.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a11de920-206f-44bc-88e5-b7bbdbe910ed;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      ":: resolution report :: resolve 113ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a11de920-206f-44bc-88e5-b7bbdbe910ed\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "24/06/05 14:30:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark1 = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"flan-t5-small\"\n",
    "EXPORT_PATH = f\"onnx_models/{MODEL_NAME}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m document_assembler \u001b[38;5;241m=\u001b[39m DocumentAssembler()\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m T5 \u001b[38;5;241m=\u001b[39m T5Transformer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_spark_nlp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline()\u001b[38;5;241m.\u001b[39msetStages([document_assembler, T5])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "T5 = T5Transformer.load(f\"{MODEL_NAME}_spark_nlp\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"summary\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline().setStages([document_assembler, T5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'question: What is a main advantage of the Rankine cycle? context: One of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.[citation needed]', 'output': 'relatively little work is required to drive the pump,'}\n"
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    input = message.value[\"input\"]\n",
    "    output = message.value[\"output\"]\n",
    "    # print(input)\n",
    "    data_df = spark1.createDataFrame([input]).toDF(\"text\")\n",
    "    results = pipeline.fit(data_df).transform(data_df)\n",
    "    results.select(\"question.result\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
