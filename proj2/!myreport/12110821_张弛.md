# STA323 Project 2 report
---
SID: 12110821
Name: ZHANG Chi

## Solution for Q1

In this assignment, we need to build a Question-Answering System based on the provided dataset and the model by fine-tunning. The dataset `squad_v2` is a collection of questions and answers, and the answer for each question is intercepted from the context that given with the question.

### (1)

After loading data by `spark.read.format("parquet").load()`, the schema should be checked first, as we only need columns regarding input and output. The schema of the dataset is shown below:

```python
root
 |-- id: string (nullable = true)
 |-- title: string (nullable = true)
 |-- context: string (nullable = true)
 |-- question: string (nullable = true)
 |-- answers: struct (nullable = true)
 |    |-- text: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- answer_start: array (nullable = true)
 |    |    |-- element: integer (containsNull = true)
```

According to the informaton of dataset provided by hugging face, the `answers` column is a struct column that contains two array columns: `text` and `answer_start`. The `text` column contains the answer to the question, and the `answer_start` column contains the starting position of the answer in the context. The `context` column contains the context of the question, and the `question` column contains the question. The `id` column is the unique identifier of the question. Here is a shortcut of the top 10 rows, with column `question` and `text`.

<img src="image/q1_1.1.png" alt="alt text" width="300" />


Before we convert them into standard inputs and outputs, I'd like to inspect how many unanswerable questions it contains, so that `size(column)` and `groupby().count()` are used. Besides, some records having missing answers text also show below.

<img src="image/q1_1.2.png" alt="alt text" width="300" />

To explore more of data, I print the length of question in ascending order, from which we could find two questions with only letter `d` are unreasonable. Hence, I will drop them in the next step. Here is shortcut of records printed by me.

<img src="image/q1_1.3.png" alt="alt text" width="300" />

Based on the data format required in the papaer `T5`, there are two columns `input` and `output` should be saved, where `input` contains both the column `question` and `context` with format `question: xxxx context: xxxx`. To concat these two fields, I use `concat()` in `withColumn()`, and the `lit("")` is necessary to add texts "question" and "context" to the string. Hence the final code is `df = df.withColumn("input", concat(lit("question: "), col("question"), lit(" context: "), col("context")))`

After preprocessing, we also need to split the training data into training set and validation set, where the number of records should be 5000 for validation set roundly. However, `randomsample()` can not designate the specific number of samples to be selected. Also, `sample()` is not used as it can not promise to choose the number we give. Therefore, to select 5000 samples exactly, `shuffle()` can be used to shuffle the data, then we can select the first 5000 samples freely. The concrete code is as follows:

```python
training_filter = training_filter.orderBy(rand(seed = 42))
training_filter.cache()
validation  = training_filter.limit(5000)
train = training_filter.exceptAll(validation)

print(train.count())   # 125317
print(validation.count())  # 5000
```
### (2)

This report outlines the process and framework used to fine-tune the Flan-T5-small model for a Question Answering (QA) task using Ray-tune for hyperparameter tuning. The whole training was performed on **openbayes with RTX4090*2**, after debugging in the course server with only CPU (using additional training and validation datasets with only 20 samples).

The training framework is divided into several key components:

-   Data Reading and Preprocessing: The whole process is wrapped in the function `load_and_preprocess_datasets()`, consisting of reading data by `load_dataset()`, tokenize the input and output text for the model in the function `preprocess_function`, and save the preprocessed data by `pickle`. To ensure that the sequences are padded and truncated to the specified lengths, parameters are set to `True`. Referring to the training examples in the huggingface repository, the `max_length` is set to 384, 30 for questions and outputs, respectively.

    ```python
        # Tokenize inputs
        model_inputs = mytokenizer(inputs, max_length=384, padding= True, truncation=True,return_tensors="pt")
        # Tokenize targets
        labels = mytokenizer(targets, max_length=30,padding = True,truncation=True,return_tensors="pt")
        # Add labels to model inputs
        model_inputs['labels'] = labels['input_ids']
    ```   
    What's more, I also define a logic that checking if there are preprocessed datasets existing and the `pickle` data can be loaded directly if true, otherwise the programme will process it and save to the designated directory passed in `config`.
-   Training Setup: Define a function `train_func()` that calls `load_and_preprocess_datasets()`, loads the model and tokenizer, sets up training arguments, and trains the model. This function will be used by Ray Tune for hyperparameter tuning. To initial tokenizer and model, I use `T5Tokenizer.from_pretraine()` and `T5ForConditionalGeneration.from_pretrained()`. Here is a pitfall that 

-   Evaluation:

-   Hyperparameter Tuning with Ray:


