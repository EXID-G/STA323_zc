{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Exercises for Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import os \n",
    "os.environ['PYSPARK_PYTHON'] = \"/root/anaconda3/bin/python\"\n",
    "\n",
    "# spark.stop()  \n",
    "## pip install kafka-python\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .config(\n",
    "    \"spark.jars\", \n",
    "    \"/shareddata/lab09/kafka-clients-3.5.0.jar,/shareddata/lab09/spark-sql-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/spark-token-provider-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/commons-pool2-2.12.0.jar\") \\\n",
    "  .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/anaconda3/bin/python\") \\\n",
    "  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "  .config(\"spark.log.level\", \"WARN\") \\\n",
    "  .appName(\"lab09_exercise\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Basic Spark Streaming Operations\n",
    "\n",
    "### (A-1): define data source\n",
    "\n",
    "Load streaming data from json files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"/shareddata/data/activity-data/\")\n",
    "dataSchema = static.schema\n",
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1).json(\"/shareddata/data/activity-data\")\n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A-2): Process data\n",
    "You can also try other operations on the data like filtering, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A-3): Define data sink and trigger, start the streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activityQuery = activityCounts.writeStream.queryName(\"activity_counts\").format(\"console\").outputMode(\"update\").trigger(processingTime=\"2 seconds\").start()\n",
    "checkpointDir = \"/data/lab/sta323/lab09_notes/ckpt\"\n",
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\").format(\"memory\").outputMode(\"update\").trigger(processingTime=\"5 seconds\").option(\"checkpointLocation\", checkpointDir).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A-4) Display data from the Table of query_name `activity_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A-5) Stop the streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.stop()\n",
    "# activityQuery.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: another example for Spark Streaming\n",
    "\n",
    "### Example 1: Simple Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simpleTransform = streaming.withColumn(\"stairs\", F.expr(\"gt like '%stairs%'\"))\\\n",
    "  .where(\"stairs\").where(\"gt is not null\")\\\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\n",
    "\n",
    "simpleTransform = simpleTransform.writeStream.queryName(\"simple_transform\").format(\"memory\").outputMode(\"append\")\n",
    "checkpointDir2 = \"/data/lab/sta323/lab09_notes/ckpt2\"\n",
    "simpleTransform = simpleTransform.trigger(processingTime=\"2 seconds\").option(\"checkpointLocation\", checkpointDir2).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleTransform.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Aggregate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "  .drop(\"avg(Arrival_time)\")\\\n",
    "  .drop(\"avg(Creation_Time)\")\\\n",
    "  .drop(\"avg(Index)\")\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM device_counts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Use socket source \n",
    "\n",
    "open a tmux window, then run the following command:\n",
    "\n",
    "* First install netcat package by `apt install netcat`\n",
    "* check the installation by `nc -h`\n",
    "* start a socket by `nc -lk 9999`\n",
    "\n",
    "Note that most ports are disabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .config(\n",
    "    \"spark.jars\", \n",
    "    \"/shareddata/lab09/kafka-clients-3.5.0.jar,/shareddata/lab09/spark-sql-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/spark-token-provider-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/commons-pool2-2.12.0.jar\") \\\n",
    "  .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/anaconda3/bin/python\") \\\n",
    "  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "  .config(\"spark.log.level\", \"WARN\") \\\n",
    "  .appName(\"lab09_exercise\").getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   F.explode(\n",
    "       F.split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = wordCounts.writeStream \\\n",
    "#     .queryName(\"word_count\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .start()\n",
    "query = wordCounts.writeStream.queryName(\"word_count\").format(\"memory\").outputMode(\"update\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The table view is valid when the stream writer uses memory format instead of concole. \n",
    "\n",
    "spark.sql(\"SELECT * FROM word_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.awaitTermination(5)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C:  Example to use Kafka \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Kafka in Spark\n",
    "\n",
    "[How to install python packages in spark driver and executor](https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html)\n",
    "\n",
    "For example, if we use the `archives` method,\n",
    "\n",
    "* first install conda-pack by `conda install conda-pack`\n",
    "* then generate the archived conda env by `conda pack -f -o pyspark_conda_env.tar.gz --ignore-missing-files`\n",
    "* Run pyspark codes by `$sparkloc/bin/spark-submit --jars /data/jupyter-data/lab08/spark-sql-kafka-0-10_2.12-3.3.1.jar,/data/jupyter-data/lab08/spark-token-provider-kafka-0-10_2.12-3.3.1.jar,/data/jupyter-data/lab08/commons-pool2-2.11.1.jar --archives /data/jupyter-data/lab08/pyspark_conda_env.tar.gz /data/jupyter-data/lab08_spark_kafka.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run kafka producer\n",
    "\n",
    "See `lab08_kafka.sh`. \n",
    "\n",
    "### Define streaming source and sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import os \n",
    "os.environ['PYSPARK_PYTHON'] = \"/root/miniconda3/envs/sp/bin/python\"\n",
    "\n",
    "# spark.stop()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .config(\n",
    "    \"spark.jars\", \n",
    "    \"/shareddata/lab09/kafka-clients-3.5.0.jar,/shareddata/lab09/spark-sql-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/spark-token-provider-kafka-0-10_2.12-3.5.0.jar, \\\n",
    "    /shareddata/lab09/commons-pool2-2.12.0.jar\") \\\n",
    "  .config(\"spark.executorEnv.PYSPARK_PYTHON\",\"/root/anaconda3/bin/python\") \\\n",
    "  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "  .config(\"spark.log.level\", \"WARN\") \\\n",
    "  .appName(\"lab09_exercise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "  .option(\"subscribe\", \"lab09\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream.queryName(\"kafka_transform\").format(\"memory\").outputMode('update').trigger(processingTime=\"5 seconds\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT get_json_object(f.value, '$.Index') as idx,get_json_object(f.value, '$.Device') as Device FROM kafka_transform f \").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT get_json_object(f.value, '$.Index') as idx,get_json_object(f.value, '$.Device') as Device FROM kafka_transform f \").tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
