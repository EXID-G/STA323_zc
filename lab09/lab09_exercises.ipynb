{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Exercises for Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"lab8_exercise\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Data is in `data/flights/departuredelays.csv`\n",
    "\n",
    "(1) Calculate the average and approximate median number of the `delay` column. \n",
    "\n",
    "(2) Group the data by `origin` and `destination` columns. Filter out the rows where their average delay time is less than or equal to zero. \n",
    "\n",
    "(3) Output the data with these three columns in the parquet format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 4.0.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py:53\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     have_arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m----> 8\u001b[0m pdf \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mpandas_api()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe average value of delay column is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelay\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe approximate median value of delay column is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelay\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5775\u001b[0m, in \u001b[0;36mDataFrame.pandas_api\u001b[0;34m(self, index_col)\u001b[0m\n\u001b[1;32m   5725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpandas_api\u001b[39m(\n\u001b[1;32m   5726\u001b[0m     \u001b[38;5;28mself\u001b[39m, index_col: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5727\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasOnSparkDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   5728\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5729\u001b[0m \u001b[38;5;124;03m    Converts the existing DataFrame into a pandas-on-Spark DataFrame.\u001b[39;00m\n\u001b[1;32m   5730\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5773\u001b[0m \u001b[38;5;124;03m    16     Bob\u001b[39;00m\n\u001b[1;32m   5774\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5775\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamespace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_index_map\n\u001b[1;32m   5776\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame \u001b[38;5;28;01mas\u001b[39;00m PandasOnSparkDataFrame\n\u001b[1;32m   5777\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InternalFrame\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/pandas/__init__.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     require_minimum_pandas_version()\n\u001b[0;32m---> 35\u001b[0m     require_minimum_pyarrow_version()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_TESTING\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/sql/pandas/utils.py:60\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_arrow:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pyarrow_version\n\u001b[1;32m     63\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pyarrow\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pyarrow_version):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pyarrow_version, pyarrow\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     68\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: PyArrow >= 4.0.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "## code here\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\")  \\\n",
    "  .option(\"inferSchema\", \"true\").load(\"data/jupyter-data/data/flights/departuredelays.csv\")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "pdf = df.pandas_api()\n",
    "print(f\"The average value of delay column is {pdf['delay'].mean()}\") \n",
    "print(f\"The approximate median value of delay column is {pdf['delay'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 9:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n",
      "|origin|destination|avg_delay|\n",
      "+------+-----------+---------+\n",
      "|   JFK|        JAC|    322.0|\n",
      "|   JAC|        JFK|    307.0|\n",
      "|   SYR|        BTV|    257.0|\n",
      "|   CRW|        DTW|    131.0|\n",
      "|   IND|        EVV|    129.0|\n",
      "+------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy('origin','destination').agg(F.mean('delay').alias('avg_delay')).filter(F.col('avg_delay') > 0).sort(F.desc('avg_delay'))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.format(\"parquet\").mode(\"overwrite\").save(\"data/jupyter-data/data/flights/avg_delay.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "The dataframe is already created. \n",
    "\n",
    "(1) Group the data by `state` column. Get the sum of `salary` column with its name as `sum_salary`. (use two methods)\n",
    "\n",
    "(2) Filter the `sum_salary` to get those who is larger than 100000. Rank the filtered `sum_salary` in descending order. (use both dataframe API and SQL operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col,sum,avg,max\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = \"employee_name STRING, department STRING, state STRING, salary INT, age INT, bonus INT\"\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1\n",
    "\n",
    "# method 1\n",
    "dfGroup = df.groupBy(\"state\").sum(\"salary\").withColumnRenamed(\"sum(salary)\", \"sum_salary\")\n",
    "dfGroup.show()\n",
    "\n",
    "\n",
    "# method2\n",
    "dfGroup=df.groupBy(\"state\").agg(F.sum(\"salary\").alias(\"sum_salary\"))\n",
    "dfGroup.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2\n",
    "\n",
    "# Use dataframe \n",
    "dfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\n",
    "dfFilter.show()\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "dfFilter.sort(desc(\"sum_salary\")).show()\n",
    "\n",
    "\n",
    "df.groupBy(\"state\") \\\n",
    "  .agg(F.sum(\"salary\").alias(\"sum_salary\")) \\\n",
    "  .filter(F.col(\"sum_salary\") > 100000)  \\\n",
    "  .sort(desc(\"sum_salary\")) \\\n",
    "  .show()\n",
    "\n",
    "\n",
    "# use SQL \n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n",
    "          \"group by state having sum_salary > 100000 \" + \n",
    "          \"order by sum_salary desc\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "Join two dataframes of `emp` and `dept`. (Inner join, use dataframe API and SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use dataframe API  \n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)\n",
    "   \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "   \n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
