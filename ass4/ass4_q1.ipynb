{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/25 13:29:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/25 13:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/25 13:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/25 13:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ass4_q1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"csv\").load(\"data/Q1_house_price_data/train.csv\", header=True, inferSchema=True)\n",
    "test = spark.read.format(\"csv\").load(\"data/Q1_house_price_data/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalCols = [x.name for x in train.schema.fields if x.dataType == IntegerType()]\n",
    "# numericalCols = [x for x in numericalCols if x != \"id\" and x != \"SalePrice\"]\n",
    "# categoricalCols = [field for (field, dataType) in train.dtypes if dataType == \"string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categorical fields are:\n",
      " ['Functional', 'MoSold', 'LotShape', 'YearBuilt', 'KitchenAbvGr', 'PavedDrive', 'FireplaceQu', 'BsmtFullBath', 'BsmtExposure', 'Electrical', 'GarageYrBlt', 'Street', 'PoolQC', 'MSSubClass', 'ExterQual', 'Foundation', 'ExterCond', 'BsmtHalfBath', 'MasVnrType', 'Alley', 'YearRemodAdd', 'SaleCondition', 'Exterior1st', 'YrSold', 'HalfBath', 'RoofStyle', 'Condition2', 'GarageFinish', 'GarageCond', 'Heating', 'OverallCond', 'OverallQual', 'CentralAir', 'MiscFeature', 'Condition1', 'BsmtQual', 'Utilities', 'MSZoning', 'BsmtCond', 'GarageQual', 'Fence', 'Neighborhood', 'KitchenQual', 'HeatingQC', 'LandSlope', 'LotConfig', 'RoofMatl', 'BsmtFinType2', 'SaleType', 'HouseStyle', 'FullBath', 'Exterior2nd', 'BldgType', 'GarageType', 'BedroomAbvGr', 'BsmtFinType1', 'LandContour']\n",
      "\n",
      "The numerical fields are:\n",
      " ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n"
     ]
    }
   ],
   "source": [
    "numericalCols = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\n",
    "\n",
    "fields = [field.name for field in train.schema]\n",
    "\n",
    "categoricalCols = list(set(fields) - set(numericalCols) - set([\"Id\",\"SalePrice\"]))\n",
    "print(\"The categorical fields are:\\n\", categoricalCols)\n",
    "print()\n",
    "print(\"The numerical fields are:\\n\",numericalCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `id` is the identifier of the record by order, which will not include much information for fitting, so that I scrap `id` in the numerical columns. \n",
    "\n",
    "filter some missing values in the numerical columns, then convert them to the `Double` datatype.\n",
    "\n",
    "``` python\n",
    "for col_name in numericalCols:\n",
    "    train = train.where(col(col_name) != \"NA\").withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "```\n",
    "\n",
    "However, this kind of methods will make the dataframe be a empty table which will cause an error `empty collection` when fitting models. The reason is that some fields in the for loop is not the string type, causing a mismatch when comparing with the string `\"NA\"`, which will return `False` uniformly in spark. To address this problem, the `NA` should be converted to the real missing value `None` firstly. Then we can feel free to filter the rows with missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_missing_values(data):\n",
    "    for numer in numericalCols:\n",
    "        data = data.withColumn(numer, when(col(numer) == \"NA\", None).otherwise(col(numer)))\n",
    "        data = data.filter(col(numer).isNotNull()).withColumn(numer, col(numer).cast(\"double\"))\n",
    "    return data\n",
    "\n",
    "pre_train = process_missing_values(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test = process_missing_values(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert the categorical columns to the datatype efficient for model to train through `StringIndexer()` and `VectorIndexer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexOutputCols = [x+\"Index\" for x in categoricalCols]\n",
    "categoricallIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "assembler = VectorAssembler(inputCols=numericalCols+indexOutputCols, outputCol=\"rawFeatures\")\n",
    "featureIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"indexedFeatures\", maxCategories=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1 (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 08:53:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt_tune = DecisionTreeRegressor(labelCol=\"SalePrice\", featuresCol=\"rawFeatures\")\n",
    "\n",
    "# 创建参数网格\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt_tune.maxDepth, [5, 10,15])\\\n",
    "    .addGrid(dt_tune.minInstancesPerNode, [1, 2, 4])\\\n",
    "    .addGrid(dt_tune.maxBins, [128,256,512]) \\\n",
    "    .build()\n",
    "    \n",
    "tvs = TrainValidationSplit(\n",
    "\testimator=dt_tune, \n",
    "    estimatorParamMaps=paramGrid,\n",
    "\tevaluator=RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"rmse\"), \n",
    "\ttrainRatio=0.8,  # 将训练集的80%用于训练模型，20%用于验证模型\n",
    "    seed = 1234\n",
    "    )\n",
    "pipeline_dt = Pipeline(stages=[categoricallIndexer, assembler,featureIndexer, tvs])\n",
    "model_dt = pipeline_dt.fit(pre_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  Id|         SalePrice|\n",
      "+----+------------------+\n",
      "|1961|120626.84541062803|\n",
      "|1962|120626.84541062803|\n",
      "|1963|120626.84541062803|\n",
      "|1964|120626.84541062803|\n",
      "|1965|176546.15384615384|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model_dt.transform(pre_test)\n",
    "\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").toPandas().to_csv(\"output/prediction1_dt.csv\", index=False)\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 08:54:33,445 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 128, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:36] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:36] task 1 got new rank 1\n",
      "2024-05-25 08:54:38,302 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2024-05-25 08:54:38,897 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-05-25 08:54:39,023 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 256, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:40] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:40] task 1 got new rank 1\n",
      "2024-05-25 08:54:42,884 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:54:43,218 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 512, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:44] task 1 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:44] task 0 got new rank 1\n",
      "2024-05-25 08:54:45,923 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2024-05-25 08:54:46,129 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:54:46,257 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 128, 'max_depth': 10, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:47] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:47] task 1 got new rank 1\n",
      "2024-05-25 08:54:50,477 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:54:50,881 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 256, 'max_depth': 10, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:52] task 1 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:52] task 0 got new rank 1\n",
      "2024-05-25 08:54:54,954 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:54:55,331 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 512, 'max_depth': 10, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:54:56] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:54:56] task 1 got new rank 1\n",
      "2024-05-25 08:54:59,145 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "24/05/25 08:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1394.7 KiB\n",
      "2024-05-25 08:54:59,405 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:54:59,538 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 128, 'max_depth': 15, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:55:00] task 1 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:55:00] task 0 got new rank 1\n",
      "2024-05-25 08:55:03,899 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2024-05-25 08:55:04,196 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:55:04,342 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 256, 'max_depth': 15, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:55:05] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:55:05] task 1 got new rank 1\n",
      "2024-05-25 08:55:09,691 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2024-05-25 08:55:09,974 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:55:10,053 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 512, 'max_depth': 15, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:55:11] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:55:11] task 1 got new rank 1\n",
      "2024-05-25 08:55:15,172 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "2024-05-25 08:55:15,810 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'max_bin': 512, 'max_depth': 5, 'objective': 'reg:squarederror', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[08:55:17] task 0 got new rank 0                                    (0 + 2) / 2]\n",
      "[08:55:17] task 1 got new rank 1\n",
      "2024-05-25 08:55:18,864 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n"
     ]
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "XGB_tune = SparkXGBRegressor(features_col=\"rawFeatures\", label_col=\"SalePrice\", num_workers=2)\n",
    "\n",
    "# 创建参数网格\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(XGB_tune.max_depth, [5, 10,15])\\\n",
    "    .addGrid(XGB_tune.max_bin, [128,256,512]) \\\n",
    "    .build()\n",
    "    \n",
    "tvs = TrainValidationSplit(\n",
    "\testimator=XGB_tune, \n",
    "    estimatorParamMaps=paramGrid,\n",
    "\tevaluator=RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"rmse\"), \n",
    "\ttrainRatio=0.8,  # 将训练集的80%用于训练模型，20%用于验证模型\n",
    "    seed = 1234\n",
    "    )\n",
    "pipeline_XGB = Pipeline(stages=[categoricallIndexer, assembler,featureIndexer, tvs])\n",
    "model_XGB = pipeline_XGB.fit(pre_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 08:55:19,626 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|  Id|     SalePrice|\n",
      "+----+--------------+\n",
      "|1961|118747.1328125|\n",
      "|1962|  95382.140625|\n",
      "|1963|    108862.875|\n",
      "|1964|116094.1015625|\n",
      "|1965|   139126.1875|\n",
      "+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n"
     ]
    }
   ],
   "source": [
    "test_pred = model_XGB.transform(pre_test)\n",
    "\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").toPandas().to_csv(\"output/prediction2_XGB.csv\", index=False)\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "s1 = SparkSession.builder.config(\"spark.log.level\", \"WARN\").config(\"spark.sql.debug.maxToStringFields\", 100).config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.4\").appName(\"Q1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msynapse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightGBMRegressor\n\u001b[0;32m----> 3\u001b[0m GBM_tune \u001b[38;5;241m=\u001b[39m LightGBMRegressor(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m\"\u001b[39m,objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 创建参数网格\u001b[39;00m\n\u001b[1;32m      6\u001b[0m paramGrid \u001b[38;5;241m=\u001b[39m ParamGridBuilder()\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39maddGrid(GBM_tune\u001b[38;5;241m.\u001b[39mmaxDepth, [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m15\u001b[39m])\\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39maddGrid(GBM_tune\u001b[38;5;241m.\u001b[39mmaxBin, [\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m]) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/synapse/ml/lightgbm/LightGBMRegressor.py:406\u001b[0m, in \u001b[0;36mLightGBMRegressor.__init__\u001b[0;34m(self, java_obj, alpha, baggingFraction, baggingFreq, baggingSeed, binSampleCount, boostFromAverage, boostingType, catSmooth, categoricalSlotIndexes, categoricalSlotNames, catl2, chunkSize, dataRandomSeed, dataTransferMode, defaultListenPort, deterministic, driverListenPort, dropRate, dropSeed, earlyStoppingRound, executionMode, extraSeed, featureFraction, featureFractionByNode, featureFractionSeed, featuresCol, featuresShapCol, fobj, improvementTolerance, initScoreCol, isEnableSparse, isProvideTrainingMetric, labelCol, lambdaL1, lambdaL2, leafPredictionCol, learningRate, matrixType, maxBin, maxBinByFeature, maxCatThreshold, maxCatToOnehot, maxDeltaStep, maxDepth, maxDrop, maxNumClasses, maxStreamingOMPThreads, metric, microBatchSize, minDataInLeaf, minDataPerBin, minDataPerGroup, minGainToSplit, minSumHessianInLeaf, modelString, monotoneConstraints, monotoneConstraintsMethod, monotonePenalty, negBaggingFraction, numBatches, numIterations, numLeaves, numTasks, numThreads, objective, objectiveSeed, otherRate, parallelism, passThroughArgs, posBaggingFraction, predictDisableShapeCheck, predictionCol, referenceDataset, repartitionByGroupingColumn, samplingMode, samplingSubsetSize, seed, skipDrop, slotNames, timeout, topK, topRate, tweedieVariancePower, uniformDrop, useBarrierExecutionMode, useMissing, useSingleDatasetMode, validationIndicatorCol, verbosity, weightCol, xGBoostDartMode, zeroAsMissing)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28msuper\u001b[39m(LightGBMRegressor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m~/anaconda3/envs/sp/lib/python3.11/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from synapse.ml.lightgbm import LightGBMRegressor\n",
    "\n",
    "GBM_tune = LightGBMRegressor(featuresCol=\"rawFeatures\", labelCol=\"SalePrice\",objective=\"regression\")\n",
    "\n",
    "# 创建参数网格\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(GBM_tune.maxDepth, [5, 10,15])\\\n",
    "    .addGrid(GBM_tune.maxBin, [128,256,512]) \\\n",
    "    .build()\n",
    "    \n",
    "tvs = TrainValidationSplit(\n",
    "\testimator=GBM_tune, \n",
    "    estimatorParamMaps=paramGrid,\n",
    "\tevaluator=RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"rmse\"), \n",
    "\ttrainRatio=0.8,  # 将训练集的80%用于训练模型，20%用于验证模型\n",
    "    seed = 1234\n",
    "    )\n",
    "pipeline_GBM = Pipeline(stages=[categoricallIndexer, assembler,featureIndexer, tvs])\n",
    "model_GBM = pipeline_GBM.fit(pre_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_GBM.transform(pre_test)\n",
    "\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").toPandas().to_csv(\"output/prediction3_GBM.csv\", index=False)\n",
    "test_pred.selectExpr(\"Id\", \"prediction as SalePrice\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
